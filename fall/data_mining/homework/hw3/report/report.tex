%%
% Copyright (c) 2017 - 2020, Pascal Wagler;
% Copyright (c) 2014 - 2020, John MacFarlane
%
% All rights reserved.
%
% Redistribution and use in source and binary forms, with or without
% modification, are permitted provided that the following conditions
% are met:
%
% - Redistributions of source code must retain the above copyright
% notice, this list of conditions and the following disclaimer.
%
% - Redistributions in binary form must reproduce the above copyright
% notice, this list of conditions and the following disclaimer in the
% documentation and/or other materials provided with the distribution.
%
% - Neither the name of John MacFarlane nor the names of other
% contributors may be used to endorse or promote products derived
% from this software without specific prior written permission.
%
% THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
% "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
% LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
% FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE
% COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
% INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
% BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
% LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
% CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
% LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN
% ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
% POSSIBILITY OF SUCH DAMAGE.
%%

%%
% This is the Eisvogel pandoc LaTeX template.
%
% For usage information and examples visit the official GitHub page:
% https://github.com/Wandmalfarbe/pandoc-latex-template
%%

% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames*,x11names*,table}{xcolor}
%
\documentclass[
  paper=a4,
,captions=tableheading
]{scrartcl}
\usepackage{lmodern}
\usepackage{setspace}
\setstretch{1.2}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\definecolor{default-linkcolor}{HTML}{A50000}
\definecolor{default-filecolor}{HTML}{A50000}
\definecolor{default-citecolor}{HTML}{4077C0}
\definecolor{default-urlcolor}{HTML}{4077C0}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Data Mining Homework 3 Report},
  hidelinks,
  breaklinks=true,
  pdfcreator={LaTeX via pandoc with the Eisvogel template}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin=2.5cm,includehead=true,includefoot=true,centering,]{geometry}
\usepackage{listings}
\newcommand{\passthrough}[1]{#1}
\lstset{defaultdialect=[5.3]Lua}
\lstset{defaultdialect=[x86masm]Assembler}
\usepackage{longtable,booktabs}
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
% add backlinks to footnote references, cf. https://tex.stackexchange.com/questions/302266/make-footnote-clickable-both-ways
\usepackage{footnotebackref}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering

% Make use of float-package and set default placement for figures to H.
% The option H means 'PUT IT HERE' (as  opposed to the standard h option which means 'You may put it here if you like').
\usepackage{float}
\floatplacement{figure}{H}


\title{Data Mining Homework 3 Report}
\date{}



%%
%% added
%%

%
% language specification
%
% If no language is specified, use English as the default main document language.
%

\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[shorthands=off,main=english]{babel}
\else
    % Workaround for bug in Polyglossia that breaks `\familydefault` when `\setmainlanguage` is used.
  % See https://github.com/Wandmalfarbe/pandoc-latex-template/issues/8
  % See https://github.com/reutenauer/polyglossia/issues/186
  % See https://github.com/reutenauer/polyglossia/issues/127
  \renewcommand*\familydefault{\sfdefault}
    % load polyglossia as late as possible as it *could* call bidi if RTL lang (e.g. Hebrew or Arabic)
  \usepackage{polyglossia}
  \setmainlanguage[]{english}
\fi



%
% for the background color of the title page
%
\usepackage{pagecolor}
\usepackage{afterpage}
\usepackage[margin=2.5cm,includehead=true,includefoot=true,centering]{geometry}

%
% break urls
%
\PassOptionsToPackage{hyphens}{url}

%
% When using babel or polyglossia with biblatex, loading csquotes is recommended
% to ensure that quoted texts are typeset according to the rules of your main language.
%
\usepackage{csquotes}

%
% captions
%
\definecolor{caption-color}{HTML}{777777}
\usepackage[font={stretch=1.2}, textfont={color=caption-color}, position=top, skip=4mm, labelfont=bf, singlelinecheck=false, justification=raggedright]{caption}
\setcapindent{0em}

%
% blockquote
%
\definecolor{blockquote-border}{RGB}{221,221,221}
\definecolor{blockquote-text}{RGB}{119,119,119}
\usepackage{mdframed}
\newmdenv[rightline=false,bottomline=false,topline=false,linewidth=3pt,linecolor=blockquote-border,skipabove=\parskip]{customblockquote}
\renewenvironment{quote}{\begin{customblockquote}\list{}{\rightmargin=0em\leftmargin=0em}%
\item\relax\color{blockquote-text}\ignorespaces}{\unskip\unskip\endlist\end{customblockquote}}

%
% Source Sans Pro as the de­fault font fam­ily
% Source Code Pro for monospace text
%
% 'default' option sets the default
% font family to Source Sans Pro, not \sfdefault.
%
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
    \usepackage[default]{sourcesanspro}
  \usepackage{sourcecodepro}
  \else % if not pdftex
    \usepackage[default]{sourcesanspro}
  \usepackage{sourcecodepro}

  % XeLaTeX specific adjustments for straight quotes: https://tex.stackexchange.com/a/354887
  % This issue is already fixed (see https://github.com/silkeh/latex-sourcecodepro/pull/5) but the
  % fix is still unreleased.
  % TODO: Remove this workaround when the new version of sourcecodepro is released on CTAN.
  \ifxetex
    \makeatletter
    \defaultfontfeatures[\ttfamily]
      { Numbers   = \sourcecodepro@figurestyle,
        Scale     = \SourceCodePro@scale,
        Extension = .otf }
    \setmonofont
      [ UprightFont    = *-\sourcecodepro@regstyle,
        ItalicFont     = *-\sourcecodepro@regstyle It,
        BoldFont       = *-\sourcecodepro@boldstyle,
        BoldItalicFont = *-\sourcecodepro@boldstyle It ]
      {SourceCodePro}
    \makeatother
  \fi
  \fi

%
% heading color
%
\definecolor{heading-color}{RGB}{40,40,40}
\addtokomafont{section}{\color{heading-color}}
% When using the classes report, scrreprt, book,
% scrbook or memoir, uncomment the following line.
%\addtokomafont{chapter}{\color{heading-color}}

%
% variables for title and author
%
\usepackage{titling}
\title{Data Mining Homework 3 Report}
\author{}

%
% tables
%

\definecolor{table-row-color}{HTML}{F5F5F5}
\definecolor{table-rule-color}{HTML}{999999}

%\arrayrulecolor{black!40}
\arrayrulecolor{table-rule-color}     % color of \toprule, \midrule, \bottomrule
\setlength\heavyrulewidth{0.3ex}      % thickness of \toprule, \bottomrule
\renewcommand{\arraystretch}{1.3}     % spacing (padding)


%
% remove paragraph indention
%
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
\setlength{\emergencystretch}{3em}  % prevent overfull lines

%
%
% Listings
%
%


%
% general listing colors
%
\definecolor{listing-background}{HTML}{F7F7F7}
\definecolor{listing-rule}{HTML}{B3B2B3}
\definecolor{listing-numbers}{HTML}{B3B2B3}
\definecolor{listing-text-color}{HTML}{000000}
\definecolor{listing-keyword}{HTML}{435489}
\definecolor{listing-keyword-2}{HTML}{1284CA} % additional keywords
\definecolor{listing-keyword-3}{HTML}{9137CB} % additional keywords
\definecolor{listing-identifier}{HTML}{435489}
\definecolor{listing-string}{HTML}{00999A}
\definecolor{listing-comment}{HTML}{8E8E8E}

\lstdefinestyle{eisvogel_listing_style}{
  language         = java,
  numbers          = left,
  xleftmargin      = 2.7em,
  framexleftmargin = 2.5em,
  backgroundcolor  = \color{listing-background},
  basicstyle       = \color{listing-text-color}\linespread{1.0}\small\ttfamily{},
  breaklines       = true,
  frame            = single,
  framesep         = 0.19em,
  rulecolor        = \color{listing-rule},
  frameround       = ffff,
  tabsize          = 4,
  numberstyle      = \color{listing-numbers},
  aboveskip        = 1.0em,
  belowskip        = 0.1em,
  abovecaptionskip = 0em,
  belowcaptionskip = 1.0em,
  keywordstyle     = {\color{listing-keyword}\bfseries},
  keywordstyle     = {[2]\color{listing-keyword-2}\bfseries},
  keywordstyle     = {[3]\color{listing-keyword-3}\bfseries\itshape},
  sensitive        = true,
  identifierstyle  = \color{listing-identifier},
  commentstyle     = \color{listing-comment},
  stringstyle      = \color{listing-string},
  showstringspaces = false,
  escapeinside     = {/*@}{@*/}, % Allow LaTeX inside these special comments
  literate         =
  {á}{{\'a}}1 {é}{{\'e}}1 {í}{{\'i}}1 {ó}{{\'o}}1 {ú}{{\'u}}1
  {Á}{{\'A}}1 {É}{{\'E}}1 {Í}{{\'I}}1 {Ó}{{\'O}}1 {Ú}{{\'U}}1
  {à}{{\`a}}1 {è}{{\'e}}1 {ì}{{\`i}}1 {ò}{{\`o}}1 {ù}{{\`u}}1
  {À}{{\`A}}1 {È}{{\'E}}1 {Ì}{{\`I}}1 {Ò}{{\`O}}1 {Ù}{{\`U}}1
  {ä}{{\"a}}1 {ë}{{\"e}}1 {ï}{{\"i}}1 {ö}{{\"o}}1 {ü}{{\"u}}1
  {Ä}{{\"A}}1 {Ë}{{\"E}}1 {Ï}{{\"I}}1 {Ö}{{\"O}}1 {Ü}{{\"U}}1
  {â}{{\^a}}1 {ê}{{\^e}}1 {î}{{\^i}}1 {ô}{{\^o}}1 {û}{{\^u}}1
  {Â}{{\^A}}1 {Ê}{{\^E}}1 {Î}{{\^I}}1 {Ô}{{\^O}}1 {Û}{{\^U}}1
  {œ}{{\oe}}1 {Œ}{{\OE}}1 {æ}{{\ae}}1 {Æ}{{\AE}}1 {ß}{{\ss}}1
  {ç}{{\c c}}1 {Ç}{{\c C}}1 {ø}{{\o}}1 {å}{{\r a}}1 {Å}{{\r A}}1
  {€}{{\EUR}}1 {£}{{\pounds}}1 {«}{{\guillemotleft}}1
  {»}{{\guillemotright}}1 {ñ}{{\~n}}1 {Ñ}{{\~N}}1 {¿}{{?`}}1
  {…}{{\ldots}}1 {≥}{{>=}}1 {≤}{{<=}}1 {„}{{\glqq}}1 {“}{{\grqq}}1
  {”}{{''}}1
}
\lstset{style=eisvogel_listing_style}

%
% Java (Java SE 12, 2019-06-22)
%
\lstdefinelanguage{Java}{
  morekeywords={
    % normal keywords (without data types)
    abstract,assert,break,case,catch,class,continue,default,
    do,else,enum,exports,extends,final,finally,for,if,implements,
    import,instanceof,interface,module,native,new,package,private,
    protected,public,requires,return,static,strictfp,super,switch,
    synchronized,this,throw,throws,transient,try,volatile,while,
    % var is an identifier
    var
  },
  morekeywords={[2] % data types
    % primitive data types
    boolean,byte,char,double,float,int,long,short,
    % String
    String,
    % primitive wrapper types
    Boolean,Byte,Character,Double,Float,Integer,Long,Short
    % number types
    Number,AtomicInteger,AtomicLong,BigDecimal,BigInteger,DoubleAccumulator,DoubleAdder,LongAccumulator,LongAdder,Short,
    % other
    Object,Void,void
  },
  morekeywords={[3] % literals
    % reserved words for literal values
    null,true,false,
  },
  sensitive,
  morecomment  = [l]//,
  morecomment  = [s]{/*}{*/},
  morecomment  = [s]{/**}{*/},
  morestring   = [b]",
  morestring   = [b]',
}

\lstdefinelanguage{XML}{
  morestring      = [b]",
  moredelim       = [s][\bfseries\color{listing-keyword}]{<}{\ },
  moredelim       = [s][\bfseries\color{listing-keyword}]{</}{>},
  moredelim       = [l][\bfseries\color{listing-keyword}]{/>},
  moredelim       = [l][\bfseries\color{listing-keyword}]{>},
  morecomment     = [s]{<?}{?>},
  morecomment     = [s]{<!--}{-->},
  commentstyle    = \color{listing-comment},
  stringstyle     = \color{listing-string},
  identifierstyle = \color{listing-identifier}
}

%
% header and footer
%
\usepackage{fancyhdr}

\fancypagestyle{eisvogel-header-footer}{
  \fancyhead{}
  \fancyfoot{}
  \lhead[]{Data Mining Homework 3 Report}
  \chead[]{}
  \rhead[Data Mining Homework 3 Report]{}
  \lfoot[\thepage]{}
  \cfoot[]{}
  \rfoot[]{\thepage}
  \renewcommand{\headrulewidth}{0.4pt}
  \renewcommand{\footrulewidth}{0.4pt}
}
\pagestyle{eisvogel-header-footer}

%%
%% end added
%%

\begin{document}

%%
%% begin titlepage
%%
\begin{titlepage}
\newgeometry{left=6cm}
\newcommand{\colorRule}[3][black]{\textcolor[HTML]{#1}{\rule{#2}{#3}}}
\begin{flushleft}
\noindent
\\[-1em]
\color[HTML]{5F5F5F}
\makebox[0pt][l]{\colorRule[435488]{1.3\textwidth}{4pt}}
\par
\noindent

{
  \setstretch{1.4}
  \vfill
  \noindent {\huge \textbf{\textsf{Data Mining Homework 3 Report}}}
    \vskip 2em
  \noindent {\Large \textsf{}}
  \vfill
}


\textsf{}
\end{flushleft}
\end{titlepage}
\restoregeometry

%%
%% end titlepage
%%



\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

Finding the most important pages on the internet has been an important
part of the success of many companies such as Google and Yahoo!. Even
today, many companies spend time making sure that their site appears at
the top of search engine results. Deciding the importance of a webiste
individually presents a difficult challenge. For this reason, the field
of link analysis, which analyzes the importance of different nodes in a
graph, takes a look at the children and parents of each node. For a
given page, the pages that link to it and the pages it links to can
inform us about the relative importance of a site. The graph of the
internet can be seen as a directed graph where each page, represented as
a node, is connected to other pages via incoming and outgoing links.

This assignment involved implementing influential link analysis
algorithms on a series of graphs, calculating the values relevant to
that algorithm. The three algorithms investigated were HITS, PageRank,
and SimRank. Following is a discussion of the algorithms, their
implementation and results, and further discussion on the strenghts and
weaknesses of each.

\hypertarget{algorithm-analysis}{%
\section{Algorithm Analysis}\label{algorithm-analysis}}

\hypertarget{hits}{%
\subsection{HITS}\label{hits}}

The HITS algorithm was one of the first algorithms to analyze pages.
This algorithm relies on a few key observations: some pages are not
authoritative in themselves, but they link to many important webpages,
i.e.~their outlinks point to other authoritative pages. The HITS
algorithm considers two factors for each page: its
\textbf{authoritativeness} and \textbf{hubness}. The former is the
measure of importance from its inlinks, while the former is the
importance of the site's outlinks.

These two factors rely on a mutual recursion \begin{equation}
    \textrm{auth}(p) = \sum_{c\in par[p]}\textrm{hub}(c)
\end{equation} and \begin{equation}
    \textrm{hub}(p) = \sum_{a\in ch[p]}\textrm{auth}(c)
\end{equation}

where \(p\) is the page in question. We implement the algorithm in an
iterative manner rather than recursive. We start of by considering a
group of nodes, and each iteration we examine the authority and hubness
of its children and parents, respectively. The algorithm stops when the
sum of the difference between the previous hubs and authorities drops
beneath a threhsold, which in this assignment is set to 1.

In our code, we initialize the authorities and hubs as an array of ones.
Then, we loop through the vertices in our graph, and update the hubness
and authority for each node \(v\). Node \(v\)'s authorities and hubness
is the sum of parent's hubs and children's authorities, respectively.
Since we update the authorities and hubs every iteration, over time this
means we are taking into account nodes farther away. The values of nodes
farther away in the graph are propagated to the parents and children
every iteration. The algorithm stops when the difference between
iteration falls below a threshold \(\epsilon\).

The authority and hubs value for node 1 using HITS can be more easily
changed by adding outlinks or inlinks respectively. By adding outlinks,
we can increase its hub score, whereas having more nodes link to 1
increases its authority score.

\hypertarget{pagerank}{%
\subsection{PageRank}\label{pagerank}}

The PageRank algorithm became one of the most recognized link analysis
algorithms due to its use in Google. PageRank does not have the idea of
hubs and authorities; rather, it defines PageRank as a function of the
parent nodes' PageRank. Specifically, for a page \(p\), its PageRank
\(r\) is defined as

\begin{equation}
    r(p) = \sum_{w\in pa[p]}\frac{r(w)}{\|ch[p]\|}
\end{equation}

We first initialize a matrix \passthrough{\lstinline!M!} by dividing
each node's row by the number of children. Thus if a node \(i\) has 4
outgoing links, all the non-zero values of row \(i\) will be
\(\frac{1}{4}\). We then initialize \(\mathbf{x}\) as the vector
containing the PageRank values for each node and normalize it.

The PageRank for a page \(p_{i}\) is given by \begin{equation}
\label{eq:PageRank}
PR(p_{i}) = \frac{d}{n}+(1-d)\sum_{l_{j,i}\in M}\frac{PR(P_{j})}{Out(p_{j})}
\end{equation}

where \(l_{i,j}\) indicates if there is a link between pages \(i\) and
\(j\). Matrix \passthrough{\lstinline!M!} already contains information
about links of each node, and \(\mathbf{x}\) contains the PageRank of
the previous iteration. Thus Equation \ref{eq:PageRank} can be
interpreted as taking the product of \passthrough{\lstinline!M!} and
\(\mathbf{x}\). The algorithm iterates until the difference in
\(\mathbf{x}\) between iterations drops beneath a threhsold, which we
interpret as converging. \(\mathbf{x}\) is the set of eigenvalues.

\hypertarget{simrank}{%
\subsection{SimRank}\label{simrank}}

This algorithm measures the relation between each pair of nodes. For
each pair of nodes \(i,j\) we loop over the array and calculate their
SimRank value for many operations. This value depends on the inlinks and
outlinks of each node. In our code we calculate the SimRank value
between each pair of nodes for a certain amount of iterations. The
\passthrough{\lstinline!sim\_scores!} matrix is first set as the
identity matrix, since each node is fully related to itself.

Afterwards, for each pair of nodes, we get \(s\) for each pair of their
parents and store the total sum as \(s(i,j)\).

\hypertarget{discussion}{%
\section{Discussion}\label{discussion}}

\hypertarget{implementation}{%
\subsection{Implementation}\label{implementation}}

The inputs to each of these algorithms is a graph of nodes. In
\passthrough{\lstinline!graph.py!} the \passthrough{\lstinline!Graph!}
class stores the parents and children for each node. It also provides
some getters as well as calculating the adjacency matrix when needed.
This graph class gets initiated when we first read the input file and
gets passed to each algorithm.

Each of the algorithms quantifies importance differently. For HITS, we
keep two vectors, one for the hubness and authorities, respectively.
However, both PageRank and SimRank keep a two dimensional matrix for
their calcualation. In our estimation, HITS utilizes less memory since
the growth in array sizes is linear with an increase in nodes.

Time efficiency is a different matter. For SimRank, we use a fixed
number of iteraitons whereas for the other algorithms we set a threshold
for termination. This way SimRank will have a more predictable run time,
but may not be as complete a description of the webpages as the other
algorithms. HITS and PageRank both set a certain threhsold depending on
the change between consecutive iterations. PageRank calculates a matrix
multiplication every iteration and normalizes \(\mathbf{x}\) to measure
the difference from the previous iteration. HITS and SimRank on the
other hand, operate on the columns and rows at one time. However, HITS
requires an operation on the hubs and authorities matrix every
iteration.

We measure the iterations taken for convergence of both PageRank and
HITS, given below.

\begin{longtable}[]{@{}lcccccc@{}}
\toprule
& graph\_1 & graph\_2 & graph\_3 & graph\_4 & graph\_5 & graph\_6 \\
\midrule
\endhead
PageRank & 38 & 7 & 7 & 6 & 4 & 4 \\
HITS & 2 & 2 & 2 & 2 & 2 & 2 \\
\bottomrule
\end{longtable}

From the graph, PageRank takes many iterations to converge from
\passthrough{\lstinline!graph\_1!}. That graph is one where nodes are
connected in a linear fashion (1\rightarrow 2,\(\dots\),5
\rightarrow 6). The threshold is set at .005 for all tests, so something
in the graph structure is causing the iterations to incrase. Increasing
the threshold to .05 results in only an average of 4 iterations for the
same graph. We believe that due to the similarity in score for each
node, the values converge at a much slower rate.

By adding extra nodes and edges between existing nodes, we can improve
the score of certain nodes. We take node 1 from graphs 1-3 as an
example. By increasing the incoming connections to node 1 from certain
other nodes, we can increase its PageRank score. In
\passthrough{\lstinline!graph\_1.txt!}, we noticed that the middle nodes
have a higher value. If we add edges between say, nodes 3 and 4 to 1, we
can slightly increase node 1's PageRank.

The method for the other files is similar. In
\passthrough{\lstinline!graph\_2.txt!} we can erase node 2's connection
to node 3 and have nodes 4 and 5 have an edge to node 1 to slightly
increase its PageRank. In \passthrough{\lstinline!graph\_3.txt!},
increasing node 1's PageRank is harder due to the strong connections of
each node. By having node 1 point to every other node and deleting most
of the nodes other nodes have with each other we can slightly increase
node 1's PageRank.

\hypertarget{further-questions}{%
\subsection{Further Questions}\label{further-questions}}

Link analysis algorithms have been an important way to find relevant
content on the web. Nevertheless, there are trade-offs and limitations
to these kinds of algorithms. The clearest limitation is the struggle
that new websites face to gain a high PageRank or SimRank value when not
many authoritative sites link to it yet. Despite the ``true'' relevance
or authoritativeness of a new website (e.g.~a new international
organization), it would take some time for other sites to link to it
before it can get a more representative score on these algorithms. In
HITS, a new site might appear as a hub, since it might link to many
auhtoritative sites without many sites pointing to it at all. New sites
would face an uphill battle to gain the score that might best reflect
their contents. Throughout the years many schemes to increase a page's
appearance on search engine results have appeared, with people creating
fake sites pointing to each other to increase the overall visibility and
then selling links to these authoritative fake sites. Despite the
potential for exploits, these algorithms have yet provided a better
browsing experience for countless people throughout the past few
decades, so their usefulness has been clearly shown.

One potential method to avoid the problem for new sites could be
estimating its initial score for any algorithm with some machine
learning model. Given a page's inlinks and outlinks we could create a
model that estimates its score based on previously seen pages. This
model could run on a given page when we index it for the first time, and
could also run periodically to remain up-to-date. Training data for such
a model would be plentiful given the variety of pages on the internet.

Whether these algorithms really find the most important pages poses an
interesting question. First, we have to define ``important'' in the
context of the internet. Do we mean popular? Scientific or medical?
Sites that hold ``important'' information might not be the most popular,
so a clear definition is important. Assuming we mean popular sites are
important, it depends on the number of inlinks and outlinks. Further
assuming that popular websites have more incoming links than outgoing,
auhtoritative websites would be the most popular, which HITS can easily
provide.

Another consideration is the computation time. Given the constantly
changing nature of the internet, having to run the algorithms constantly
on such big matrices quickly becomes expensive. For a company looking to
use these algorithms, the size of the graph could play a role in
choosing one algorithm over another.

\hypertarget{conclusion}{%
\subsection{Conclusion}\label{conclusion}}

This assignment focused on implementing some of the most important link
analysis algorithms, such as HITS, PageRank, and SimRank. All of these
algorithms have contributed to the way we naviagate around networks such
as the internet, although they can be used for other networks, such as
citation networks. All of these algorithms view the number of incoming
and outgoing links to determine the importance of a page. By
implementing these algorithms on different graphs, we were able to see
how the graph structure influences the behavior of the algorithms and
how each page's scores can be increased.

Furthermore, we explored some of the downsides of these algorithms and
the challenges they face during deployment. The modern web, with so many
pages and connections, presents a large challenge in implementing these
algorithms. With newer techniques, the size of the internet might not be
the issue it was before, since a model can estimate the score of a page
based on the pages it has seen previously.

With the internet and other networks playing an increasing role in our
lives, finding the most relevant information is of great importance.
However, before we do so we need to recognize important information. The
connections we can draw from seeing connections in graphs can lead to
even more insights about how we categorize data.

\end{document}
