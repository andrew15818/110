%%
% Copyright (c) 2017 - 2020, Pascal Wagler;
% Copyright (c) 2014 - 2020, John MacFarlane
%
% All rights reserved.
%
% Redistribution and use in source and binary forms, with or without
% modification, are permitted provided that the following conditions
% are met:
%
% - Redistributions of source code must retain the above copyright
% notice, this list of conditions and the following disclaimer.
%
% - Redistributions in binary form must reproduce the above copyright
% notice, this list of conditions and the following disclaimer in the
% documentation and/or other materials provided with the distribution.
%
% - Neither the name of John MacFarlane nor the names of other
% contributors may be used to endorse or promote products derived
% from this software without specific prior written permission.
%
% THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
% "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
% LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
% FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE
% COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
% INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
% BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
% LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
% CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
% LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN
% ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
% POSSIBILITY OF SUCH DAMAGE.
%%

%%
% This is the Eisvogel pandoc LaTeX template.
%
% For usage information and examples visit the official GitHub page:
% https://github.com/Wandmalfarbe/pandoc-latex-template
%%

% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames*,x11names*,table}{xcolor}
%
\documentclass[
  paper=a4,
,captions=tableheading
]{scrartcl}
\usepackage{lmodern}
\usepackage{setspace}
\setstretch{1.2}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\definecolor{default-linkcolor}{HTML}{A50000}
\definecolor{default-filecolor}{HTML}{A50000}
\definecolor{default-citecolor}{HTML}{4077C0}
\definecolor{default-urlcolor}{HTML}{4077C0}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Data Mining Homework 1 Report},
  hidelinks,
  breaklinks=true,
  pdfcreator={LaTeX via pandoc with the Eisvogel template}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin=2.5cm,includehead=true,includefoot=true,centering,]{geometry}
\usepackage{listings}
\newcommand{\passthrough}[1]{#1}
\lstset{defaultdialect=[5.3]Lua}
\lstset{defaultdialect=[x86masm]Assembler}
\usepackage{longtable,booktabs}
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
% add backlinks to footnote references, cf. https://tex.stackexchange.com/questions/302266/make-footnote-clickable-both-ways
\usepackage{footnotebackref}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering

% Make use of float-package and set default placement for figures to H.
% The option H means 'PUT IT HERE' (as  opposed to the standard h option which means 'You may put it here if you like').
\usepackage{float}
\floatplacement{figure}{H}


\title{Data Mining Homework 1 Report}
\date{}



%%
%% added
%%

%
% language specification
%
% If no language is specified, use English as the default main document language.
%

\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[shorthands=off,main=english]{babel}
\else
    % Workaround for bug in Polyglossia that breaks `\familydefault` when `\setmainlanguage` is used.
  % See https://github.com/Wandmalfarbe/pandoc-latex-template/issues/8
  % See https://github.com/reutenauer/polyglossia/issues/186
  % See https://github.com/reutenauer/polyglossia/issues/127
  \renewcommand*\familydefault{\sfdefault}
    % load polyglossia as late as possible as it *could* call bidi if RTL lang (e.g. Hebrew or Arabic)
  \usepackage{polyglossia}
  \setmainlanguage[]{english}
\fi



%
% for the background color of the title page
%
\usepackage{pagecolor}
\usepackage{afterpage}
\usepackage[margin=2.5cm,includehead=true,includefoot=true,centering]{geometry}

%
% break urls
%
\PassOptionsToPackage{hyphens}{url}

%
% When using babel or polyglossia with biblatex, loading csquotes is recommended
% to ensure that quoted texts are typeset according to the rules of your main language.
%
\usepackage{csquotes}

%
% captions
%
\definecolor{caption-color}{HTML}{777777}
\usepackage[font={stretch=1.2}, textfont={color=caption-color}, position=top, skip=4mm, labelfont=bf, singlelinecheck=false, justification=raggedright]{caption}
\setcapindent{0em}

%
% blockquote
%
\definecolor{blockquote-border}{RGB}{221,221,221}
\definecolor{blockquote-text}{RGB}{119,119,119}
\usepackage{mdframed}
\newmdenv[rightline=false,bottomline=false,topline=false,linewidth=3pt,linecolor=blockquote-border,skipabove=\parskip]{customblockquote}
\renewenvironment{quote}{\begin{customblockquote}\list{}{\rightmargin=0em\leftmargin=0em}%
\item\relax\color{blockquote-text}\ignorespaces}{\unskip\unskip\endlist\end{customblockquote}}

%
% Source Sans Pro as the de­fault font fam­ily
% Source Code Pro for monospace text
%
% 'default' option sets the default
% font family to Source Sans Pro, not \sfdefault.
%
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
    \usepackage[default]{sourcesanspro}
  \usepackage{sourcecodepro}
  \else % if not pdftex
    \usepackage[default]{sourcesanspro}
  \usepackage{sourcecodepro}

  % XeLaTeX specific adjustments for straight quotes: https://tex.stackexchange.com/a/354887
  % This issue is already fixed (see https://github.com/silkeh/latex-sourcecodepro/pull/5) but the
  % fix is still unreleased.
  % TODO: Remove this workaround when the new version of sourcecodepro is released on CTAN.
  \ifxetex
    \makeatletter
    \defaultfontfeatures[\ttfamily]
      { Numbers   = \sourcecodepro@figurestyle,
        Scale     = \SourceCodePro@scale,
        Extension = .otf }
    \setmonofont
      [ UprightFont    = *-\sourcecodepro@regstyle,
        ItalicFont     = *-\sourcecodepro@regstyle It,
        BoldFont       = *-\sourcecodepro@boldstyle,
        BoldItalicFont = *-\sourcecodepro@boldstyle It ]
      {SourceCodePro}
    \makeatother
  \fi
  \fi

%
% heading color
%
\definecolor{heading-color}{RGB}{40,40,40}
\addtokomafont{section}{\color{heading-color}}
% When using the classes report, scrreprt, book,
% scrbook or memoir, uncomment the following line.
%\addtokomafont{chapter}{\color{heading-color}}

%
% variables for title and author
%
\usepackage{titling}
\title{Data Mining Homework 1 Report}
\author{}

%
% tables
%

\definecolor{table-row-color}{HTML}{F5F5F5}
\definecolor{table-rule-color}{HTML}{999999}

%\arrayrulecolor{black!40}
\arrayrulecolor{table-rule-color}     % color of \toprule, \midrule, \bottomrule
\setlength\heavyrulewidth{0.3ex}      % thickness of \toprule, \bottomrule
\renewcommand{\arraystretch}{1.3}     % spacing (padding)


%
% remove paragraph indention
%
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
\setlength{\emergencystretch}{3em}  % prevent overfull lines

%
%
% Listings
%
%


%
% general listing colors
%
\definecolor{listing-background}{HTML}{F7F7F7}
\definecolor{listing-rule}{HTML}{B3B2B3}
\definecolor{listing-numbers}{HTML}{B3B2B3}
\definecolor{listing-text-color}{HTML}{000000}
\definecolor{listing-keyword}{HTML}{435489}
\definecolor{listing-keyword-2}{HTML}{1284CA} % additional keywords
\definecolor{listing-keyword-3}{HTML}{9137CB} % additional keywords
\definecolor{listing-identifier}{HTML}{435489}
\definecolor{listing-string}{HTML}{00999A}
\definecolor{listing-comment}{HTML}{8E8E8E}

\lstdefinestyle{eisvogel_listing_style}{
  language         = java,
  numbers          = left,
  xleftmargin      = 2.7em,
  framexleftmargin = 2.5em,
  backgroundcolor  = \color{listing-background},
  basicstyle       = \color{listing-text-color}\linespread{1.0}\small\ttfamily{},
  breaklines       = true,
  frame            = single,
  framesep         = 0.19em,
  rulecolor        = \color{listing-rule},
  frameround       = ffff,
  tabsize          = 4,
  numberstyle      = \color{listing-numbers},
  aboveskip        = 1.0em,
  belowskip        = 0.1em,
  abovecaptionskip = 0em,
  belowcaptionskip = 1.0em,
  keywordstyle     = {\color{listing-keyword}\bfseries},
  keywordstyle     = {[2]\color{listing-keyword-2}\bfseries},
  keywordstyle     = {[3]\color{listing-keyword-3}\bfseries\itshape},
  sensitive        = true,
  identifierstyle  = \color{listing-identifier},
  commentstyle     = \color{listing-comment},
  stringstyle      = \color{listing-string},
  showstringspaces = false,
  escapeinside     = {/*@}{@*/}, % Allow LaTeX inside these special comments
  literate         =
  {á}{{\'a}}1 {é}{{\'e}}1 {í}{{\'i}}1 {ó}{{\'o}}1 {ú}{{\'u}}1
  {Á}{{\'A}}1 {É}{{\'E}}1 {Í}{{\'I}}1 {Ó}{{\'O}}1 {Ú}{{\'U}}1
  {à}{{\`a}}1 {è}{{\'e}}1 {ì}{{\`i}}1 {ò}{{\`o}}1 {ù}{{\`u}}1
  {À}{{\`A}}1 {È}{{\'E}}1 {Ì}{{\`I}}1 {Ò}{{\`O}}1 {Ù}{{\`U}}1
  {ä}{{\"a}}1 {ë}{{\"e}}1 {ï}{{\"i}}1 {ö}{{\"o}}1 {ü}{{\"u}}1
  {Ä}{{\"A}}1 {Ë}{{\"E}}1 {Ï}{{\"I}}1 {Ö}{{\"O}}1 {Ü}{{\"U}}1
  {â}{{\^a}}1 {ê}{{\^e}}1 {î}{{\^i}}1 {ô}{{\^o}}1 {û}{{\^u}}1
  {Â}{{\^A}}1 {Ê}{{\^E}}1 {Î}{{\^I}}1 {Ô}{{\^O}}1 {Û}{{\^U}}1
  {œ}{{\oe}}1 {Œ}{{\OE}}1 {æ}{{\ae}}1 {Æ}{{\AE}}1 {ß}{{\ss}}1
  {ç}{{\c c}}1 {Ç}{{\c C}}1 {ø}{{\o}}1 {å}{{\r a}}1 {Å}{{\r A}}1
  {€}{{\EUR}}1 {£}{{\pounds}}1 {«}{{\guillemotleft}}1
  {»}{{\guillemotright}}1 {ñ}{{\~n}}1 {Ñ}{{\~N}}1 {¿}{{?`}}1
  {…}{{\ldots}}1 {≥}{{>=}}1 {≤}{{<=}}1 {„}{{\glqq}}1 {“}{{\grqq}}1
  {”}{{''}}1
}
\lstset{style=eisvogel_listing_style}

%
% Java (Java SE 12, 2019-06-22)
%
\lstdefinelanguage{Java}{
  morekeywords={
    % normal keywords (without data types)
    abstract,assert,break,case,catch,class,continue,default,
    do,else,enum,exports,extends,final,finally,for,if,implements,
    import,instanceof,interface,module,native,new,package,private,
    protected,public,requires,return,static,strictfp,super,switch,
    synchronized,this,throw,throws,transient,try,volatile,while,
    % var is an identifier
    var
  },
  morekeywords={[2] % data types
    % primitive data types
    boolean,byte,char,double,float,int,long,short,
    % String
    String,
    % primitive wrapper types
    Boolean,Byte,Character,Double,Float,Integer,Long,Short
    % number types
    Number,AtomicInteger,AtomicLong,BigDecimal,BigInteger,DoubleAccumulator,DoubleAdder,LongAccumulator,LongAdder,Short,
    % other
    Object,Void,void
  },
  morekeywords={[3] % literals
    % reserved words for literal values
    null,true,false,
  },
  sensitive,
  morecomment  = [l]//,
  morecomment  = [s]{/*}{*/},
  morecomment  = [s]{/**}{*/},
  morestring   = [b]",
  morestring   = [b]',
}

\lstdefinelanguage{XML}{
  morestring      = [b]",
  moredelim       = [s][\bfseries\color{listing-keyword}]{<}{\ },
  moredelim       = [s][\bfseries\color{listing-keyword}]{</}{>},
  moredelim       = [l][\bfseries\color{listing-keyword}]{/>},
  moredelim       = [l][\bfseries\color{listing-keyword}]{>},
  morecomment     = [s]{<?}{?>},
  morecomment     = [s]{<!--}{-->},
  commentstyle    = \color{listing-comment},
  stringstyle     = \color{listing-string},
  identifierstyle = \color{listing-identifier}
}

%
% header and footer
%
\usepackage{fancyhdr}

\fancypagestyle{eisvogel-header-footer}{
  \fancyhead{}
  \fancyfoot{}
  \lhead[]{Data Mining Homework 1 Report}
  \chead[]{}
  \rhead[Data Mining Homework 1 Report]{}
  \lfoot[\thepage]{}
  \cfoot[]{}
  \rfoot[]{\thepage}
  \renewcommand{\headrulewidth}{0.4pt}
  \renewcommand{\footrulewidth}{0.4pt}
}
\pagestyle{eisvogel-header-footer}

%%
%% end added
%%

\begin{document}

%%
%% begin titlepage
%%
\begin{titlepage}
\newgeometry{left=6cm}
\newcommand{\colorRule}[3][black]{\textcolor[HTML]{#1}{\rule{#2}{#3}}}
\begin{flushleft}
\noindent
\\[-1em]
\color[HTML]{5F5F5F}
\makebox[0pt][l]{\colorRule[435488]{1.3\textwidth}{4pt}}
\par
\noindent

{
  \setstretch{1.4}
  \vfill
  \noindent {\huge \textbf{\textsf{Data Mining Homework 1 Report}}}
    \vskip 2em
  \noindent {\Large \textsf{}}
  \vfill
}


\textsf{}
\end{flushleft}
\end{titlepage}
\restoregeometry

%%
%% end titlepage
%%



\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

Finding patterns in large, unstructured data such as a transaction
database might at first seem too computationally expensive to perform.
Our goal is to find items that occur often together and make inferences
about them. In the end, those subsets which appear more than a certain
threshold (called its \textbf{support}) are considered frequent
itemsets. From frequent itemsets we can generate \textbf{association
rules} by calculating how often the subsets of frequent itemsets occur
together.

If we wanted to find frequent patterns in a set of transactions, a naive
approach would be to go through each transaction of the database, and
count the appearance of all the subsets in each transaction. Such an
approach would be prohibitive in terms of memory and time. It also
misses a key insight: all subsets of a frequent itemset are themselves
frequent. This is called the \textbf{a-priori} property, and it is the
basis of the two algorithms compared in this report. Using this
property, if at any point we encounter a set that is not frequent, we
know that all its supersets are also not frequent.

After frequent itemsets are calcualted, association rules are generated
which measure the chance of two sets of items occurring together. For
instance, the rule A \Rightarrow B means that whenever \(A\) occurs,
\(B\) also occurs with a certain probability \(p\), called the rule's
\textbf{confidence}.

To generate association rules from frequent itemsets, various algorithms
have been proposed. The focus of this report is comparing the
\textbf{a-priori} algorithm and the \textbf{FP-Growth} algorithm.

\hypertarget{a-priori-algorithm}{%
\subsection{A-priori Algorithm}\label{a-priori-algorithm}}

The a-priori algorithm uses the a-priori property to build up the
frequent itemsets. First, the \(1\)-itemsets are found by scanning the
dataset \(D\) and getting the support count for each item; only the
items with high enough support are kept.

Next, until the frequent itemsets \(L\) is empty, we keep generating the
new candidate set \(C_{k+1}\). From the set of frequent itemsets
\(L_{k}\), we combine each element \(l_{1}\) with an element from
\(l_{2}\) if \(l_{2}\)'s last element is greater than \(l_{1}\)'s last
element. Having this requirement preserves the lexicographical order of
the itemsets.

We maintain a structure called a \textbf{hash tree}, whose leaf nodes
contain the itemsets and their counts and the internal nodes contain the
hashes of the children nodes. To insert a new item, we check the hash of
a certain index of the item, then insert the item to the corresponding
bucket in the node.

When the length of a bucket exceeds a certain amount, we have to split
the items in the node's buckets into different children corresponding to
its hash value. By recursively inserting items and splitting the nodes,
eventually by following the hash of each index of the itemset we reach
the leaf node that contains its count or where it should be inserted.
The hash tree structure allows us to maintain a count and insert items
in a more efficient way.

To get the frequent itemsets, we check all the leaf nodes in the hash
table that contain a support count greater than the minimum support. The
frequent itemsets are the \(k\)-itemsets \(L_{k}\). We repeat this
process until there are no more new frequent itemsets, or
\(L_{k} = \emptyset\).

\hypertarget{fp-growth-algorithm}{%
\subsection{FP-Growth Algorithm}\label{fp-growth-algorithm}}

The FP-Growth algorithm addresses some of the potential drawbacks of the
a-priori algorithm. The first algorithm requires several scans of the
database to determine the support count of datasets. Furthermore, for
large databases the space required to store the hash tree might be
prohibitive as well.

The FP-Growth algorithm uses a compressed form of the database to
determine support counts for the itemsets, and avoids the candidate
generation approach which might produce a large amount of itemsets
without enough support. First we determine the support for the
\(1\)-itemsets in the same way as in a-priori: by scanning the database
and maintaining the support count of each element in the transactions.

After we determine \(L_{1}\), we proceed to build the \textbf{fp-tree}
as follows. First, we loop again through the dataset, sort each
transaction in decreasing order according to its support count in
\(L_{1}\) and insert them into the tree. If the item at a given index in
the transaction is present in the node's children, we increase its count
and recurse on that node, checking increasing the index. In the end, we
are left with a tree of the most common items close to the root and less
common items closer to the leaves. Since there might be more than one
node with the item ``1'' for example, we keep a table where we maintain
a reference to all the nodes for ``1'', as well as for all the other
items.

The header table becomes useful in the next step which involves building
the \textbf{conditional pattern bases}. For each item, e.g.~``1'', we
build the set of all the paths from these nodes to the roots. We then
make smaller fp-tree, called the \textbf{conditional fp-tree}, for each
element and insert the items in the conditional pattern base to the
conditional fp-tree, and maintain the support count. Once we build the
conditional pattern tree for an item \(i\), we prune the nodes who do
not meet the minimal support count. The remaining items in the tree are
the items that comprise the frequent itemsets involving item \(i\).

The overall set of frequent itemsets is thus the concatenation of the
frequent itemsets for each item \(i\).

\hypertarget{experiment}{%
\subsection{Experiment}\label{experiment}}

To test the differnce between the two algorithms, we test several
transitional datasets and compare the execution time and results. There
are a couple of datasets we are using: a 9-item file with the textbook
example, used to check for correct itemset generation; a 100-item
\passthrough{\lstinline!.data!} file produced by the IBM dataset
generator; and the provided \passthrough{\lstinline!ibm-5000.txt!} file.
The last set of 25,000 items is found in the
\passthrough{\lstinline!ibm-2021.txt!} file.

The results are the average of three runs. The testing results for the
a-priori with a support count of 2 and rule confidence of 0.8 algorithm
are as follows

\begin{longtable}[]{@{}lll@{}}
\toprule
Trans. No. & Memory Usage(Mb) & Time(s) \\
\midrule
\endhead
\textbf{9} & 12.993 & .000185 \\
\textbf{100} & 12.902 & .0024 \\
\textbf{4798} & 17.529 & .2013 \\
\textbf{25,000} & 39.165 & 3.4646 \\
\bottomrule
\end{longtable}

For the FP-Growth, the corresponding tests yield the following results

\begin{longtable}[]{@{}lll@{}}
\toprule
Trans. No. & Memory Usage(Mb) & Time(s) \\
\midrule
\endhead
\textbf{9} & 12.998 & .0003 \\
\textbf{100} & 13.418 & .0056 \\
\textbf{4798} & 18.388 & .0046 \\
\textbf{25,000} & 38.997 & .4982 \\
\bottomrule
\end{longtable}

\hypertarget{performance-analysis}{%
\subsection{Performance Analysis}\label{performance-analysis}}

The datasets can also affect the performance of the two algorithms. For
instance, the 100-item dataset used an average transaction length of 3
items, however the number of customers was much higher than the number
of transactions. This lead to overall lower confidence in each
individual rule and a fewer amount of rules in total, e.g.~customer 1000
would have only a few appearances in a smaller dataset. In a realistic
setting adjusting the support and confidence requirements in accordance
with the dataset yield the best results; however, for this experiment
those settings were left constant for all tests. Having a higher minimum
support and/or confidence requirement would increase the computation
speed since it would result in fewer frequent itemsets, however this
could come at the cost of missing potentially useful relationships.
Thus, apart from using a more efficient algorithm, other parameters
could be adjusted for optimal results.

The motivation for the FP-Growth algorithm involved reducing the number
of database scans to also reduce the computational complexity. This
advantage becomes clearer the larger the amount of transactions in the
dataset. Even though the memory requirements remained close (possibly a
result of the implementation), the time taken to generate frequent
patterns and rules in the FP-Growth algorithm is much lower than
a-priori for larger datasets. Again, avoiding costly database scans in
favor of repeated tree scans can certainly help depending on the
structure of the conditional pattern and conditional growth trees.

In terms of memory, both implementations required a similar amount of
memory during the experiments. While some of the reason might lie in the
implementation details(e.g.~built-in structures used for some steps),
having a large hashtree or fp-tree would also contribute to the memory
requirements. Doing a very quick calculation, for a file containing
around \(100,000\) unique items (such as
\passthrough{\lstinline!ibm-2021.txt!}), \(20\%\) of the items being
considered frequent, each frequent item appearing on average \(14\)
times {[}\^{}1{]}, times 64-bits per number and the result is a tree of
estimated size

\begin{equation}
100000 \times 0.20 \times 14 \times 64 = 17.920\textrm{Mb}
\end{equation}

Although with a low support threshold, it is likely that a larger
portion of the dataset is considered ``frequent'', which would lead to a
much larger tree. This example is nevertheless useful in seeing how
repeated items in our tree could lead to a bigger tree size.

\hypertarget{support-and-confidence}{%
\subsection{Support and Confidence}\label{support-and-confidence}}

Finding association rules \(A \Rightarrow B\) from the set of frequent
itemsets \(F_{K}\) requires finding the support of \(AB\). The formula
for calculating the confidence in a given rule is \begin{equation}
confidence(A \Rightarrow B) = \frac{support(A \cup B)}{support(A)}
\end{equation}

In the algorithms, \(A\cup B\) is usually a single frequent itemset
\(f_{k}\), from which we derive the length \(k\) subsets that make up
the right and left hand sides of the rule. Since every subset of a
frequent itemset is itself frequent, we know that both \(A\) and \(B\)
are frequent itemsets. However, the relationship between their
individual support counts can lead to interesting results.

As the support of \(A\cup B\) and \(A\) both go higher, this will lead
to a high support count, but the confidence will actually be lower,
possibly being rejected. If \(A\cup B\) has a higher support count and
\(A\) has a lower support count (i.e.~their support counts do not differ
that much), the support for \(A\cup B\) will be higher and the
confidence as well. If \(A\cup B\) has lower support count, and \(A\)'s
support is higher relative to it, the confidence will be lower because
of the larger denominator, making this rule a candidate to be dropped.
Finally, as the support count of \(A\cup B\) and \(A\) both tend towards
lower values, the support count will be lower but the confidence could
turn out to be high.

\hypertarget{conclusion}{%
\subsection{Conclusion}\label{conclusion}}

Determining which items occur together in a set of transactions can be
quite a complex task, especially with thousands or even millions of
individual transactions. Frequent patterns could help many kinds of
businesses know which items should be bundled together in a store, for
example, or help in creating enticing discounts. Sifting through large
amounts of data to find these groups of items would quickly become
prohibitive with a naive approach.

In this assignment, both the a-priori and FP-Growth algorithms were
tested in terms of computation time and memory requirements. The
a-priori algorithm iteratively builds a candidate item list \(C_{k}\) to
determine the \(k\)-length itemsets \(L_{k}\). It uses a hash-tree
structure to store the itemsets and their counts as leaf nodes for
quickly determining subsets. This approach can generate many candidates
that don't result in frequent itemsets.

To address this issue, the fp-algorithm uses a compressed version of the
database to determine frequent itemsets. By using an fp-tree, we can
determine which items are common prefixes of item \(i\) and use those to
generate frequent itemsets. This approach results in a more efficient
approach since the entire database is stored only once, the computation
being mostly done recursively on the fp-tree.

This assignment focused on applying them to a transactional setting;
however, many other kinds of scenarios are possible where other methods
might be more appropriate. Nevertheless, the usefulness of these
algorithms is evident in their wide use and influence in the field. With
more general methods, even better and more efficient results could be
achieved from a dataset. {[}\^{}1{]}: Taken by sorting the count of each
item.

\end{document}
