%%
% Copyright (c) 2017 - 2020, Pascal Wagler;
% Copyright (c) 2014 - 2020, John MacFarlane
%
% All rights reserved.
%
% Redistribution and use in source and binary forms, with or without
% modification, are permitted provided that the following conditions
% are met:
%
% - Redistributions of source code must retain the above copyright
% notice, this list of conditions and the following disclaimer.
%
% - Redistributions in binary form must reproduce the above copyright
% notice, this list of conditions and the following disclaimer in the
% documentation and/or other materials provided with the distribution.
%
% - Neither the name of John MacFarlane nor the names of other
% contributors may be used to endorse or promote products derived
% from this software without specific prior written permission.
%
% THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
% "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
% LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
% FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE
% COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
% INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
% BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
% LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
% CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
% LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN
% ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
% POSSIBILITY OF SUCH DAMAGE.
%%

%%
% This is the Eisvogel pandoc LaTeX template.
%
% For usage information and examples visit the official GitHub page:
% https://github.com/Wandmalfarbe/pandoc-latex-template
%%

% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames*,x11names*,table}{xcolor}
%
\documentclass[
  paper=a4,
,captions=tableheading
]{scrartcl}
\usepackage{lmodern}
\usepackage{setspace}
\setstretch{1.2}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\definecolor{default-linkcolor}{HTML}{A50000}
\definecolor{default-filecolor}{HTML}{A50000}
\definecolor{default-citecolor}{HTML}{4077C0}
\definecolor{default-urlcolor}{HTML}{4077C0}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  hidelinks,
  breaklinks=true,
  pdfcreator={LaTeX via pandoc with the Eisvogel template}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin=2.5cm,includehead=true,includefoot=true,centering,]{geometry}
\usepackage{listings}
\newcommand{\passthrough}[1]{#1}
\lstset{defaultdialect=[5.3]Lua}
\lstset{defaultdialect=[x86masm]Assembler}
% add backlinks to footnote references, cf. https://tex.stackexchange.com/questions/302266/make-footnote-clickable-both-ways
\usepackage{footnotebackref}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering

% Make use of float-package and set default placement for figures to H.
% The option H means 'PUT IT HERE' (as  opposed to the standard h option which means 'You may put it here if you like').
\usepackage{float}
\floatplacement{figure}{H}


\date{}



%%
%% added
%%

%
% language specification
%
% If no language is specified, use English as the default main document language.
%

\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[shorthands=off,main=english]{babel}
\else
    % Workaround for bug in Polyglossia that breaks `\familydefault` when `\setmainlanguage` is used.
  % See https://github.com/Wandmalfarbe/pandoc-latex-template/issues/8
  % See https://github.com/reutenauer/polyglossia/issues/186
  % See https://github.com/reutenauer/polyglossia/issues/127
  \renewcommand*\familydefault{\sfdefault}
    % load polyglossia as late as possible as it *could* call bidi if RTL lang (e.g. Hebrew or Arabic)
  \usepackage{polyglossia}
  \setmainlanguage[]{english}
\fi



%
% for the background color of the title page
%

%
% break urls
%
\PassOptionsToPackage{hyphens}{url}

%
% When using babel or polyglossia with biblatex, loading csquotes is recommended
% to ensure that quoted texts are typeset according to the rules of your main language.
%
\usepackage{csquotes}

%
% captions
%
\definecolor{caption-color}{HTML}{777777}
\usepackage[font={stretch=1.2}, textfont={color=caption-color}, position=top, skip=4mm, labelfont=bf, singlelinecheck=false, justification=raggedright]{caption}
\setcapindent{0em}

%
% blockquote
%
\definecolor{blockquote-border}{RGB}{221,221,221}
\definecolor{blockquote-text}{RGB}{119,119,119}
\usepackage{mdframed}
\newmdenv[rightline=false,bottomline=false,topline=false,linewidth=3pt,linecolor=blockquote-border,skipabove=\parskip]{customblockquote}
\renewenvironment{quote}{\begin{customblockquote}\list{}{\rightmargin=0em\leftmargin=0em}%
\item\relax\color{blockquote-text}\ignorespaces}{\unskip\unskip\endlist\end{customblockquote}}

%
% Source Sans Pro as the de­fault font fam­ily
% Source Code Pro for monospace text
%
% 'default' option sets the default
% font family to Source Sans Pro, not \sfdefault.
%
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
    \usepackage[default]{sourcesanspro}
  \usepackage{sourcecodepro}
  \else % if not pdftex
    \usepackage[default]{sourcesanspro}
  \usepackage{sourcecodepro}

  % XeLaTeX specific adjustments for straight quotes: https://tex.stackexchange.com/a/354887
  % This issue is already fixed (see https://github.com/silkeh/latex-sourcecodepro/pull/5) but the
  % fix is still unreleased.
  % TODO: Remove this workaround when the new version of sourcecodepro is released on CTAN.
  \ifxetex
    \makeatletter
    \defaultfontfeatures[\ttfamily]
      { Numbers   = \sourcecodepro@figurestyle,
        Scale     = \SourceCodePro@scale,
        Extension = .otf }
    \setmonofont
      [ UprightFont    = *-\sourcecodepro@regstyle,
        ItalicFont     = *-\sourcecodepro@regstyle It,
        BoldFont       = *-\sourcecodepro@boldstyle,
        BoldItalicFont = *-\sourcecodepro@boldstyle It ]
      {SourceCodePro}
    \makeatother
  \fi
  \fi

%
% heading color
%
\definecolor{heading-color}{RGB}{40,40,40}
\addtokomafont{section}{\color{heading-color}}
% When using the classes report, scrreprt, book,
% scrbook or memoir, uncomment the following line.
%\addtokomafont{chapter}{\color{heading-color}}

%
% variables for title and author
%
\usepackage{titling}
\title{}
\author{}

%
% tables
%

%
% remove paragraph indention
%
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
\setlength{\emergencystretch}{3em}  % prevent overfull lines

%
%
% Listings
%
%


%
% general listing colors
%
\definecolor{listing-background}{HTML}{F7F7F7}
\definecolor{listing-rule}{HTML}{B3B2B3}
\definecolor{listing-numbers}{HTML}{B3B2B3}
\definecolor{listing-text-color}{HTML}{000000}
\definecolor{listing-keyword}{HTML}{435489}
\definecolor{listing-keyword-2}{HTML}{1284CA} % additional keywords
\definecolor{listing-keyword-3}{HTML}{9137CB} % additional keywords
\definecolor{listing-identifier}{HTML}{435489}
\definecolor{listing-string}{HTML}{00999A}
\definecolor{listing-comment}{HTML}{8E8E8E}

\lstdefinestyle{eisvogel_listing_style}{
  language         = java,
  numbers          = left,
  xleftmargin      = 2.7em,
  framexleftmargin = 2.5em,
  backgroundcolor  = \color{listing-background},
  basicstyle       = \color{listing-text-color}\linespread{1.0}\small\ttfamily{},
  breaklines       = true,
  frame            = single,
  framesep         = 0.19em,
  rulecolor        = \color{listing-rule},
  frameround       = ffff,
  tabsize          = 4,
  numberstyle      = \color{listing-numbers},
  aboveskip        = 1.0em,
  belowskip        = 0.1em,
  abovecaptionskip = 0em,
  belowcaptionskip = 1.0em,
  keywordstyle     = {\color{listing-keyword}\bfseries},
  keywordstyle     = {[2]\color{listing-keyword-2}\bfseries},
  keywordstyle     = {[3]\color{listing-keyword-3}\bfseries\itshape},
  sensitive        = true,
  identifierstyle  = \color{listing-identifier},
  commentstyle     = \color{listing-comment},
  stringstyle      = \color{listing-string},
  showstringspaces = false,
  escapeinside     = {/*@}{@*/}, % Allow LaTeX inside these special comments
  literate         =
  {á}{{\'a}}1 {é}{{\'e}}1 {í}{{\'i}}1 {ó}{{\'o}}1 {ú}{{\'u}}1
  {Á}{{\'A}}1 {É}{{\'E}}1 {Í}{{\'I}}1 {Ó}{{\'O}}1 {Ú}{{\'U}}1
  {à}{{\`a}}1 {è}{{\'e}}1 {ì}{{\`i}}1 {ò}{{\`o}}1 {ù}{{\`u}}1
  {À}{{\`A}}1 {È}{{\'E}}1 {Ì}{{\`I}}1 {Ò}{{\`O}}1 {Ù}{{\`U}}1
  {ä}{{\"a}}1 {ë}{{\"e}}1 {ï}{{\"i}}1 {ö}{{\"o}}1 {ü}{{\"u}}1
  {Ä}{{\"A}}1 {Ë}{{\"E}}1 {Ï}{{\"I}}1 {Ö}{{\"O}}1 {Ü}{{\"U}}1
  {â}{{\^a}}1 {ê}{{\^e}}1 {î}{{\^i}}1 {ô}{{\^o}}1 {û}{{\^u}}1
  {Â}{{\^A}}1 {Ê}{{\^E}}1 {Î}{{\^I}}1 {Ô}{{\^O}}1 {Û}{{\^U}}1
  {œ}{{\oe}}1 {Œ}{{\OE}}1 {æ}{{\ae}}1 {Æ}{{\AE}}1 {ß}{{\ss}}1
  {ç}{{\c c}}1 {Ç}{{\c C}}1 {ø}{{\o}}1 {å}{{\r a}}1 {Å}{{\r A}}1
  {€}{{\EUR}}1 {£}{{\pounds}}1 {«}{{\guillemotleft}}1
  {»}{{\guillemotright}}1 {ñ}{{\~n}}1 {Ñ}{{\~N}}1 {¿}{{?`}}1
  {…}{{\ldots}}1 {≥}{{>=}}1 {≤}{{<=}}1 {„}{{\glqq}}1 {“}{{\grqq}}1
  {”}{{''}}1
}
\lstset{style=eisvogel_listing_style}

%
% Java (Java SE 12, 2019-06-22)
%
\lstdefinelanguage{Java}{
  morekeywords={
    % normal keywords (without data types)
    abstract,assert,break,case,catch,class,continue,default,
    do,else,enum,exports,extends,final,finally,for,if,implements,
    import,instanceof,interface,module,native,new,package,private,
    protected,public,requires,return,static,strictfp,super,switch,
    synchronized,this,throw,throws,transient,try,volatile,while,
    % var is an identifier
    var
  },
  morekeywords={[2] % data types
    % primitive data types
    boolean,byte,char,double,float,int,long,short,
    % String
    String,
    % primitive wrapper types
    Boolean,Byte,Character,Double,Float,Integer,Long,Short
    % number types
    Number,AtomicInteger,AtomicLong,BigDecimal,BigInteger,DoubleAccumulator,DoubleAdder,LongAccumulator,LongAdder,Short,
    % other
    Object,Void,void
  },
  morekeywords={[3] % literals
    % reserved words for literal values
    null,true,false,
  },
  sensitive,
  morecomment  = [l]//,
  morecomment  = [s]{/*}{*/},
  morecomment  = [s]{/**}{*/},
  morestring   = [b]",
  morestring   = [b]',
}

\lstdefinelanguage{XML}{
  morestring      = [b]",
  moredelim       = [s][\bfseries\color{listing-keyword}]{<}{\ },
  moredelim       = [s][\bfseries\color{listing-keyword}]{</}{>},
  moredelim       = [l][\bfseries\color{listing-keyword}]{/>},
  moredelim       = [l][\bfseries\color{listing-keyword}]{>},
  morecomment     = [s]{<?}{?>},
  morecomment     = [s]{<!--}{-->},
  commentstyle    = \color{listing-comment},
  stringstyle     = \color{listing-string},
  identifierstyle = \color{listing-identifier}
}

%
% header and footer
%
\usepackage{fancyhdr}

\fancypagestyle{eisvogel-header-footer}{
  \fancyhead{}
  \fancyfoot{}
  \lhead[]{}
  \chead[]{}
  \rhead[]{}
  \lfoot[\thepage]{}
  \cfoot[]{}
  \rfoot[]{\thepage}
  \renewcommand{\headrulewidth}{0.4pt}
  \renewcommand{\footrulewidth}{0.4pt}
}
\pagestyle{eisvogel-header-footer}

%%
%% end added
%%

\begin{document}

%%
%% begin titlepage
%%

%%
%% end titlepage
%%



\hypertarget{classification}{%
\section{Classification}\label{classification}}

Machine learning just refers to a program \(P\) that learns with some
experience \(E\) according to some way of measuring its performance
\(P\).

\textbf{Supervised} learning has labels along with the training data.
This means that durign training we can compare our guess with the
correct answer.

\textbf{Unsupervised} data's correct answer is unknown, and here we want
to discover the undrelying relations or clusters within the data.

\hypertarget{definition}{%
\subsection{Definition}\label{definition}}

Given a set of attributes (among which there is a class label), our
model is a function of the attributes and outputs a class label. Before
we start processing the data, some data cleaning in order to normalize
the values or fill in missing values might be required.

There are several ways of evaluating the performance of our model: -
predictive accuracy - speed(inference, etc\ldots) - robustness (how does
it handle missing values?) - scalability - interpretability (how easy is
it to understand conceptually what it's doing?) - goodness of rules
(size, compactness of the representation)

some techniques include: - Decision Trees - Neural networks - K-nearest
neighbors - Support Vector Machines - Random forest

\hypertarget{decision-trees}{%
\subsection{Decision Trees}\label{decision-trees}}

DTs are a hierarchical way of assigning class labels based on checking
an attribute at a certain value.

The general way we build the decision trees is by having \(D_{T}\) at
some node. If the items in the dataset are all one class, we assign the
data's label to the node. If there is data belonging to more than one
set, we use some metric to split the dataset at the node. Some trees
could be split into two features, or we could choose to have a
multi-split approach.

If the values in a feature are continuous, how do we get a value to
split on? We could choose the average of two consecutive sorted values
in the feature column. We then have multiple different splits of data,
so which is the best one? Based on the proposed value, we need some
metric of checking the \textbf{purity} of the resulting datasets. There
are a couple of metrics:

The \textbf{gini index} measures the impurity of a node \begin{equation}
    \textrm{GINI}(t) = 1 - \sum_{j}[p(j|t)]^{2}
\end{equation} where \(p(j|t)\) is the relative measure of items in
class \(j\) in set \(t\), i.e.~\(|c_{j\in t}| / |t|\). When all the
items belong to one class, the gini index is 0, so we are choosing the
dataset that is the least ``mixed''. Thus at every node, we have to
calculate the gini index of both the resulting children nodes and
subtract it from the impurity of the parent.

Another measure we could use is the \textbf{entropy}, which measures the
homogeneity of a node. \begin{equation}
    \textrm{Entropy}(t) = -\sum_{j}p(j|t)\log p(j|t)
\end{equation}

Yet another measure is the \textbf{information gain}, which measures the
\emph{reduction in entropy} of the resulting split. \begin{equation}
\textrm{GAIN}_{split} = \textrm{Entropy}(p) - (\sum_{i=1}^{k} \frac{n_{i}}{n}\textrm{Entropy}(i))
\end{equation} However, this apporach tends to prefer smaller and more
numerous yet purer partitions.

The \textbf{error} is just the probability that the label is different
from our prediction, \(E(t) = 1 - \maxP(i|t)\).

Decision trees are a popular method for classification because they are
easy to understand, deal well with symbolic feature, relatively quick to
train, and deal well with data that is not numeric.

\hypertarget{classification-issues}{%
\subsection{Classification Issues}\label{classification-issues}}

When our model is too simple or too complicated, the accuracy in our
predictions suffers. Why? If our model is too simple, whe nwe take a
look at the training data, our error will still be relatively large when
we finish training. This means that our model cannot entirely capture
all the information in the training data. The error in the training data
will be large, and the testing data's error will be even larger.

When our model is too complicated, it might focus too much on generating
useful parameters for the testing set, but not on the testing set. It
might be getting derailed by noise in the testing data. Its accuracy on
the training data might be very low, but it might be very high on the
testing data due to it not being general enough to handle random noise.
It might split the space into too small areas.

How should we deal with overfitting? \textbf{Early stopping} has the
algorithm stop before convergence, (e.g. stopping before the decision
tree is completed). Each problem is going to require us to come up with
different conditions for stopping.

We could also do the opposite and prune the tree/model once it finishes
if there are any nodes we believe are too much.

For decision trees, if we could test for multiple attributes at a single
node we could reach a consensus faster, although finding the right
combination of attributes would be too expensive. Another issue with DTs
is that subtrees might be replicated at different parts.

The \textbf{curse of dimensionality} refers to how quickly the
dimensions of our data escalates. For a simple boolean expression
\((A\and B) \or (C \and D)\), there would be \(2^{param\_no}\) possible
inputs. For non-boolean data this increase would be exponentially
greater.

\hypertarget{instance-based-classifiers}{%
\subsection{Instance-Based
Classifiers}\label{instance-based-classifiers}}

With these classifiers, we use the nearby attributes to classify each
node. \textbf{k-nearest neighbors} takes a look at the closest data
points and decides the class based on the nearby data points.

To classify the points in the test set, we need the training data's
attributes and labels, and a distance metric, and the number of the
neighbors to check to make our decision. \textbf{Lazy learners} such as
knn do not build models explicitly during a training phase, and instead
do the classification ``on the fly'' for each test point.

Choosing the value of \(k\) is important because if it is too small, it
can be influenced by noise, and if it is too big it can also take into
account items from another class. Since some of the values can be
larger, to keep them from dominating the entire prediction we have to
\textbf{normalize} them (e.g. people's heights changes by couple
centimeters vs.~income can vary by thousands). Another application is to
normalize the input vectors to \textbf{unit length}.

\hypertarget{bayes-classifier}{%
\subsection{Bayes Classifier}\label{bayes-classifier}}

This classifier is based on Bayes theorem \begin{equation}
    P(C|A) = \frac{P(A|C)P(C)}{P(A)}
\end{equation} i.e.~how often an event \(C\) happens given \(A\) is the
relationship between how often they happen together, how likely \(C\) is
on its own, and how often \(A\) happens on its own. If \(A\) happens
frequently, the chance that they happen together is pretty high.

To find the chance an input \(A\) belongs to class \(C\), we have to
find the prob \(P(a_{1}, a_{2}, \dots,a_{n}|C)\), or how likely the
attributes are given they belong to class \(C\). We can estimate that
all fo the attributes are indpendent and normally distributed.

\hypertarget{perceptron}{%
\subsection{Perceptron}\label{perceptron}}

This model takes in inputs, and passes them through a weighted directed
graph. The result is the collection of weights, biases and activations
of the nodes at each previous layer. They all lead to the final outputs.
When we train a perceptron, we are changing the weights of the neurons
used to calculate the output of the neurons.

\hypertarget{bias-variance-tradeoff}{%
\subsection{Bias-Variance Tradeoff}\label{bias-variance-tradeoff}}

\textbf{Bias} refers to the ``offset'' from otherwise the answer we
would get. The \textbf{variance} refers to how much our answers vary
from the mean. The higher the variance, the more sporadic the values
might seem.

There is a U-turn in the variance-bias relationship. First, as our model
is more complex, the variance starts to go up. However, the bias is more
influential when the model is simpler. The trick is making our model
complex enough that we minimize the bias and the variance just starts to
increase. At this point, right after the variance starts picking up, is
the best time to stop our gradient descent. When there is \textbf{high
bias,low variance} our model will tend to \textbf{underfit}, and when
there is \textbf{low bias, high variance}, our model will tend to
\textbf{overfit}.

\hypertarget{support-vector-machines}{%
\subsection{Support Vector Machines}\label{support-vector-machines}}

With SVMs, given 2-dimensional data, we ask whether there is a
\textbf{hyperplane} linearly separating the data. There are multiple
lines that linearly separate the data, which one is better? The better
line is the one that maximizes the distance between the line and the
closest point, a.k.a. the \textbf{margin}.

We would have to maximize the margin \begin{equation}
    \textrm{Margin} = \frac{1}{||w||^{2}}
\end{equation}

which means we want to minimize \(||w||\).

However, if the data is not linearly separable or the hyperplane is not
linear this approach will not work. In that case we would have to
transform the data into a higher dimension such that it is linearly
separable with a \(d-1\) dimensional hyperplane. We would have to apply
some sort of transformation to the input data to find the hyperplane
that best separates the data.

\hypertarget{ensemble-methods}{%
\subsection{Ensemble methods}\label{ensemble-methods}}

The ensemble here refers to a collection of other classifiers, whose
consensus we take as the final class. There are a couple ways of dealing
with the classifiers: we can take their combined output. Given an error
rate \(\epsilon = 0.35\), the chance that our set makes a wrong
prediction is \begin{equation}
    E(x) = \sum_{i=13}^{25}\frac{25}{i}\epsilon^{i}(1-\epsilon)^{(25-i)}
\end{equation}

This helps when an individual classifier might have a large error rate
or when a small change in the input can have a large effect on the
output. With \textbf{boosting}, we have a set of base classifiers which
try to learn from the mistakes of the previous classifiers. The samples
that the previous classifier got wrong are given higher weights to make
sure they are improved on in future classifiers. Each data sample is
\((x, y, w)\) where \(w\) is the weight.

\hypertarget{supervised-semi-supervised-unsupervised}{%
\subsection{Supervised, Semi-Supervised,
Unsupervised}\label{supervised-semi-supervised-unsupervised}}

Labeling data is expensive and time-consuming, so often we have just
unlabeled data to analyze. The types of unsupervised learning depend on
whether we have some labeled data or not. Why is having unlabeled data
helpful? If we only have a few labeled samples, but we have a lot of
data, those few samples could be enough to approximate the distribution.

Even if we only have a few labeled examples, we can infer and propagate
the labels by looking at each point's similarity to the labeled data.
Even if we only have two points, give each point a label based on
closest one.

\textbf{Positive examples} refer to examples of a class \(P\).
\textbf{PU} learning happens when we have a combination of positive and
unlabeled examples. Our goal with such a classifier is to accurately
classify the unlabeled samples, however we don't have \textbf{negative
data}.

As an example to classify academic documents, the \textbf{Spy} algorithm
will use a mixture of unlabeled and positive labeled samples to build a
classifier, and try to infer the reliable negatives against the samples
we already know.

\end{document}
