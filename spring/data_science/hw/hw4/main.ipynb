{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fbeb7ec-697d-406a-bc49-75d982115940",
   "metadata": {},
   "source": [
    "# Data Science Homework 4\n",
    "Try different imbalanced classification datasets using $k$-fold cross validation and various classification methods.\n",
    "TODO List:\n",
    "- Make sure we can open all the data as either DataFrame or nparray\n",
    "- Handle categorical data (tokenize, one-hot encoding, ....)\n",
    "- Split each dataset into training and testing dataset.\n",
    "- Perform any necessary sampling, imputaiton, encoding techniques depending on dataset\n",
    "- Perform 5-fold cross-validation to select datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3197ea94-1d37-4113-baf2-de091cac66a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost\n",
    "import lightgbm\n",
    "import neuralnet # My simple nn\n",
    "import sklearn.svm as svm\n",
    "import sklearn.metrics as metrics\n",
    "import category_encoders as ce # sklearn library\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "DATA_DIR = './data'\n",
    "files = os.listdir(DATA_DIR)\n",
    "if '.ipynb_checkpoints' in files:\n",
    "    files.remove('.ipynb_checkpoints')\n",
    "\n",
    "special_delims = { 'arcene_train.data': ' '}\n",
    "no_headers = ['covtype.data', 'arcene_train.data']\n",
    "\n",
    "numMethods = ['clip',\n",
    "              'standard',\n",
    "              'minmax',\n",
    "              'bin',\n",
    "              'rank'\n",
    "            ]\n",
    "ceMethods = [#'onehot',\n",
    "              'label',\n",
    "              #'feature',\n",
    "              #'target',\n",
    "              #'leaveoneout',\n",
    "              #'frequency'\n",
    "             ]\n",
    "modelNames = [#'forest',\n",
    "             #'xgboost',\n",
    "             #'lightgbm',\n",
    "             'mlp',\n",
    "             #'svm'\n",
    "            ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4815b9fb-5923-4120-9df7-0bbf3ffe073a",
   "metadata": {},
   "source": [
    "## Load datasets\n",
    "Some datasets are in csv format, others have just the data. First convert to `DataFrame`s to allow for numeric, categorical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66cd7606-affa-4a6b-a7de-265d2703dc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load(name, header=True):\n",
    "    sep = special_delims[name] if name in special_delims else ','\n",
    "    name = os.path.join(DATA_DIR, name)\n",
    "    if header:\n",
    "        df = pd.read_csv(name, sep=sep)\n",
    "    else:\n",
    "        df = pd.read_csv(name, header=None, sep=sep)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f6ed6f-5619-4bd8-bd2b-ec3f64277958",
   "metadata": {},
   "source": [
    "## Encode Categorical Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01a3a02f-649f-45ea-9efc-a9c014157915",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_per_column(data, encoder):\n",
    "    for column in data.columns:\n",
    "        data[column] = encoder.fit_transform(data[column])\n",
    "    return data\n",
    "        \n",
    "def encode_categorical(data:pd.DataFrame, method='ordinal'):\n",
    "    if method == 'ordinal':\n",
    "        encoder = ce.OrdinalEncoder()\n",
    "    elif method == 'onehot':\n",
    "        encoder = ce.OneHotEncoder()\n",
    "    elif method == 'label':\n",
    "        encoder = preprocessing.LabelEncoder()\n",
    "        return encode_per_column(data,encoder)\n",
    "    elif method == 'feature':\n",
    "        encoder = FeatureHasher(n_features=10, input_type='string')\n",
    "        encoder.transform(data.type)\n",
    "    elif method == 'target':\n",
    "        encoder = ce.target_encoder.TargetEncoder()\n",
    "        y = data.iloc[:,-1]\n",
    "        return encoder.fit_transform(data.iloc[:,:-1], y)\n",
    "    elif method == 'leaveoneout':\n",
    "        y = data.iloc[:,-1]\n",
    "        encoder = ce.LeaveOneOutEncoder()\n",
    "        return encoder.fit_transform(data.iloc[:,:-1], y)\n",
    "    elif method == 'frequency':\n",
    "        encoder = ce.CountEncoder()\n",
    "    return encoder.fit_transform(data)\n",
    "\n",
    "def scale_features(data:pd.DataFrame, method='standard') -> pd.DataFrame:\n",
    "    if method == 'standard':\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "    elif method == 'minmax':\n",
    "        scaler = preprocessing.MinMaxScaler()\n",
    "    elif method == 'clip':\n",
    "        for column in data.columns:\n",
    "            p01 = data[column].quantile(.01)\n",
    "            p99 = data[column].quantile(.99)\n",
    "            data[column].clip(p01, p99)\n",
    "        return data\n",
    "    elif method == 'bin':\n",
    "        for column in data.columns:\n",
    "            data[column] = pd.cut(data[column], 5, labels=False, duplicates='drop')\n",
    "        return data\n",
    "    elif method == 'rank':\n",
    "        for column in data.columns:\n",
    "            data[column] = data[column].rank()\n",
    "        return data\n",
    "    \n",
    "    return scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5deb65dd-2d12-4851-9783-037c307c731b",
   "metadata": {},
   "source": [
    "## Train model:\n",
    "The evalutation should be fixed on 5-fold cross validation, choose from `RandomForest`, `GBDT`, `XGBoost`,`LightBGM`, `CatBoost`, `KNN`, `Logistic Regression`,`MLP`, `SVM`.\n",
    "\n",
    "Train a new model every iteration of the cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f52818bb-a52f-4e49-90d3-aa1008e90157",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_model(method):\n",
    "    if method == 'xgboost':\n",
    "        model = xgboost.XGBClassifier(5)\n",
    "    elif method == 'knn':\n",
    "        model = KNeighborsClassifier()\n",
    "    elif method == 'forest':\n",
    "        model = RandomForestClassifier(10)\n",
    "    elif method == 'lightgbm':\n",
    "        model = lgb\n",
    "    elif method == 'mlp':\n",
    "        model = neuralnet.tabularNet()\n",
    "    elif method == 'svm':\n",
    "        model = svm.SVC()\n",
    "    else:\n",
    "        print(f'{method} not supported.')\n",
    "        model = None\n",
    "    return model\n",
    "\n",
    "def get_metric(method):\n",
    "    if method == 'acc':\n",
    "        met = metrics.accuracy_score\n",
    "    elif method == 'auc':\n",
    "        met = metrics.auc\n",
    "    elif method == 'roc_auc':\n",
    "        met = metrics.roc_auc_score\n",
    "    elif method == 'f1':\n",
    "        met = metrics.f1_score\n",
    "    return met\n",
    "\n",
    "def train(x, y, method='xgboost', metric='acc'):\n",
    "    # Perform K-Fold cross validation\n",
    "    metric = get_metric(metric)\n",
    "    n_items = x.shape[0]\n",
    "    kf = KFold( n_splits=5, shuffle=True)\n",
    "    \n",
    "    mean_score = 0 \n",
    "    for i, (train_index, test_index) in enumerate(kf.split(x)):\n",
    "        model = get_model(method)\n",
    "        x_train, x_test = x.iloc[train_index,: ], x.iloc[test_index, :]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        if y_train.min().item()  != 0 and method == 'xgboost':\n",
    "            y_train -= y_train.min().item() # Class labels should be zero-based\n",
    "        \n",
    "        model.fit(x_train, y_train)\n",
    "        \n",
    "        # Evaluate\n",
    "        preds = model.predict(x_test)\n",
    "        score = metric(y_test, preds)\n",
    "        mean_score += score\n",
    "        #print(f'\\t{mean_score}')\n",
    "        \n",
    "    mean_score /= (i+1)\n",
    "    #print(f'Mean score over validation: {mean_score} using {method}')\n",
    "\n",
    "    return mean_score\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570aa398-74ca-476c-ab9c-1b703a1cae20",
   "metadata": {},
   "source": [
    "## P1: How does feature scaling (e.g. performing normalization) affect performance?\n",
    "With standardization we apply the formula\n",
    "$$x' = \\frac{x - \\mu}{\\sigma}$$\n",
    "so that we have 0 mean in the training data.\n",
    "\n",
    "For One-hot Encoding we should not use this with tree-based models. For these models we can use label encoding, feature-hashing.\n",
    "We should first encode the categorical values, then train the models on all our datasets both with feature scaling and without."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d8aa7e-001d-4e4b-9b41-400240cce3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    \n",
    "    df = load(file, False if file in no_headers else True)\n",
    "    # Encode categorical columns\n",
    "    cats = df.select_dtypes(include=['object'])\n",
    "    df[cats.columns] = encode_categorical(cats)\n",
    "    \n",
    "    control = df.copy(deep=True) # Compare with standardized dataset\n",
    "\n",
    "    df, labels = df.iloc[:, :-1], df.iloc[:, -1]\n",
    "    control, c_labels = control.iloc[:, :-1], control.iloc[:, -1]\n",
    "    \n",
    "    # Feature scaling\n",
    "    df = pd.DataFrame(scale_features(df))\n",
    "    \n",
    "    # Train model\n",
    "    print(f'Training on {file}')\n",
    "    score = train(df, labels, method='knn', metric='acc')\n",
    "    print(f'train: {file}: avg: {score}')\n",
    "   \n",
    "    score = train(control, c_labels, method='knn', metric='acc')\n",
    "    print(f'test: {file}: avg: {score}')\n",
    "    print('\\n')\n",
    "    # The performance is mostly the same across models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13aad0b2",
   "metadata": {},
   "source": [
    "## P2 When using tree-based algorithms, will usng one-hot encoding for categorical features generate worse performance than using label encoding? Why?\n",
    "One-hot encoding transforms inputs with a range of $(1,\\dots,n)$. This method generates extra columns and usually results in a sparse matrix.\n",
    "\n",
    "Label Encoding adds replaces each unique class name with an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bc1a45-e699-414f-8259-00d78398f75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    df = load(file, False if file in no_headers else True)\n",
    "    target_column = df.columns[-1]\n",
    "    label = df.copy(deep=True) # Use label encoding\n",
    "    \n",
    "    cats = df.select_dtypes(include=['object'])\n",
    "\n",
    "    # Ignore files w/ no categorical data, otherw encoder error\n",
    "    if cats.empty:\n",
    "        print(f'No categorical in {file}\\n')\n",
    "        continue\n",
    "        \n",
    "    n_unique = df[target_column].unique().shape[0] # Num of unique items in target column\n",
    "\n",
    "    one_hot = pd.get_dummies(df)\n",
    "    cats = label.select_dtypes(include=['object'])\n",
    "    #print(f'{file}: {target_column}')\n",
    "    # Apply Label Encoder to each column\n",
    "    for column in cats.columns:\n",
    "        label[column] = encode_categorical(label[column], method='label')\n",
    "        \n",
    "    # If target column is not categorical, labels will only be one column\n",
    "    if df.dtypes[target_column] == object:\n",
    "        n_unique = df[target_column].unique().shape[0]\n",
    "    else:\n",
    "        n_unique = 1\n",
    "    \n",
    "    x_ohe, y_ohe = one_hot.iloc[:,:-n_unique], one_hot.iloc[:,-n_unique:]\n",
    "    x_label, y_label = label.iloc[:,:-1], label.iloc[:,-1]\n",
    "    \n",
    "    #print(f'{file}: xcols: {len(x_label)} ycols: {len(y_label)}')\n",
    "    #print(one_hot)\n",
    "    print(f'Training on {file}')\n",
    "    score = train(x_ohe, y_ohe, method='forest', metric='acc')\n",
    "    print(f'train: {file}: avg: {score}')\n",
    "    \n",
    "    score = train(x_label, y_label, method='forest', metric='acc')\n",
    "    print(f'test: {file}: avg: {score}')\n",
    "    print('\\n')\n",
    "    ## There is a slight difference in performance (label encoder usually better by a bit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9237acb9-d3f7-46f9-8114-72f6010104c9",
   "metadata": {},
   "source": [
    "## P3\n",
    "Will feature binning provide performance improvement? When is binning useful (which models or which kinds of datasets)?\n",
    "\n",
    "Loop through each column in the dataframe and bin individually.\n",
    "We use LabelEncoder for categorical features on both DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dcdbe8-67ee-4764-8212-9f24ea33daa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    df = load(file, False if file in no_headers else True)\n",
    "    binned = df.copy(deep=True)\n",
    "    # TODO: fix arcene random space unable to parse\n",
    "    if file == 'arcene_train.data':\n",
    "        continue\n",
    "    # Bin each column individually, use LabelEncoder for categorical\n",
    "    for column in binned.columns:\n",
    "        if (binned[column].dtype.name == 'int64'\n",
    "           or binned[column].dtype.name == 'float64'):\n",
    "            #n_unique = binned[column].unique() \n",
    "            binned[column]= pd.cut(binned[column], 5, labels=False, duplicates='drop')\n",
    "            #binned[column].fillna(0, inplace=True)\n",
    "\n",
    "            if binned[column].isnull().values.any():\n",
    "                binned[colum] = 0\n",
    "        #elif check if column is categorical\n",
    "        elif binned[column].dtype.name == 'object':\n",
    "            binned[column] = encode_categorical(binned[column], method='label')\n",
    "            df[column] = encode_categorical(df[column], method='ordinal')\n",
    "\n",
    "    # Train\n",
    "    x_bin, y_bin = binned.iloc[:, :-1], binned.iloc[:, -1]\n",
    "    x_label, y_label = df.iloc[:, :-1], df.iloc[:, -1]\n",
    "    \n",
    "    print(f'Training on {file}')\n",
    "    score = train(x_bin, y_bin, method='forest', metric='acc')\n",
    "    print(f'train: {file}: avg: {score}')\n",
    "    \n",
    "    score = train(x_label, y_label, method='forest', metric='acc')\n",
    "    print(f'test: {file}: avg: {score}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2e941f-c7b6-49e2-9f0b-db9271a52a80",
   "metadata": {},
   "source": [
    "## P4\n",
    "Compare the performance of 6 different categorical feature encoding methods based on Random Forest, XGBoost LightGBM, MLP, SVM. Which of the 6 encoding methods is better?\n",
    "TODO: Finish implementing neural network (missing dataloader)\n",
    "\n",
    "Models to test: \n",
    "1. One hot encoding\n",
    "2. Label Encoding\n",
    "3. Feature Hasing\n",
    "4. Frequency Encoding\n",
    "5. Target Encoding\n",
    "6. Leave One Out Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8671b37-af75-4c5f-850e-b0194e58641d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    print(f'Checking {file}')\n",
    "    df = load(file, header = False if file in no_headers else True)\n",
    "    cats = df.select_dtypes(include=['object'])\n",
    "    for method in ceMethods:\n",
    "        # Encode features\n",
    "        encoded = df.copy()\n",
    "        \n",
    "        try:\n",
    "            encode_categorical(encoded, method=method)\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        for modelName in modelNames:\n",
    "            # Number of class columns, necessary for OHE\n",
    "            labelIndex = len(df.iloc[:,-1].unique()) if method == 'onehot' else 1\n",
    "            enc_train, enc_test = encoded.iloc[:,:-labelIndex], encoded.iloc[:,-labelIndex:]\n",
    "            score = train(enc_train, enc_test, modelName)\n",
    "            print(f'\\t({method}, {modelName}): {score}')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d41521-2aa7-4487-afb4-05ab54bc0265",
   "metadata": {},
   "source": [
    "## P5\n",
    "Which combinations of numerical and categorical feature transformation methods generally lead to better results?\n",
    "\n",
    "Here we can test the different numerical and categorical methods on each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41309118-82c9-45a7-901d-f5486a76c1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clip\n",
      "standard\n",
      "minmax\n",
      "bin\n",
      "rank\n",
      "clip\n",
      "standard\n",
      "minmax\n",
      "bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29824/3632305457.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[column] = pd.cut(data[column], 5, labels=False, duplicates='drop')\n",
      "/tmp/ipykernel_29824/3632305457.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[column] = data[column].rank()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank\n",
      "clip\n",
      "standard\n",
      "minmax\n",
      "bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29824/3632305457.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[column] = pd.cut(data[column], 5, labels=False, duplicates='drop')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29824/3632305457.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[column] = data[column].rank()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clip\n",
      "standard\n",
      "minmax\n",
      "bin\n",
      "rank\n",
      "clip\n",
      "standard\n",
      "minmax\n",
      "bin\n",
      "rank\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29824/3632305457.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[column] = pd.cut(data[column], 5, labels=False, duplicates='drop')\n",
      "/tmp/ipykernel_29824/3632305457.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[column] = data[column].rank()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clip\n",
      "standard\n",
      "minmax\n",
      "bin\n",
      "rank\n",
      "clip\n",
      "standard\n",
      "minmax\n",
      "bin\n",
      "rank\n",
      "clip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29824/3632305457.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[column] = pd.cut(data[column], 5, labels=False, duplicates='drop')\n",
      "/tmp/ipykernel_29824/3632305457.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[column] = data[column].rank()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standard\n",
      "minmax\n",
      "bin\n",
      "rank\n",
      "clip\n",
      "standard\n",
      "minmax\n",
      "bin\n",
      "rank\n",
      "clip\n",
      "standard\n",
      "minmax\n",
      "bin\n",
      "rank\n",
      "clip\n",
      "standard\n",
      "minmax\n",
      "bin\n",
      "rank\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29824/3632305457.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[column] = pd.cut(data[column], 5, labels=False, duplicates='drop')\n",
      "/tmp/ipykernel_29824/3632305457.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[column] = data[column].rank()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clip\n",
      "standard\n",
      "minmax\n",
      "bin\n",
      "rank\n",
      "clip\n",
      "standard\n",
      "minmax\n",
      "bin\n",
      "rank\n",
      "clip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29824/3632305457.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[column] = pd.cut(data[column], 5, labels=False, duplicates='drop')\n",
      "/tmp/ipykernel_29824/3632305457.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[column] = data[column].rank()\n",
      "/tmp/ipykernel_29824/3632305457.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[column] = pd.cut(data[column], 5, labels=False, duplicates='drop')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standard\n",
      "minmax\n",
      "bin\n",
      "rank\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29824/3632305457.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[column] = data[column].rank()\n"
     ]
    },
    {
     "ename": "EmptyDataError",
     "evalue": "No columns to parse from file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEmptyDataError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m files:\n\u001b[0;32m----> 2\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mno_headers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m numMethod \u001b[38;5;129;01min\u001b[39;00m numMethods:\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28mprint\u001b[39m(numMethod)\n",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, header)\u001b[0m\n\u001b[1;32m      5\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(name, sep\u001b[38;5;241m=\u001b[39msep)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m----> 7\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    666\u001b[0m     dialect,\n\u001b[1;32m    667\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    677\u001b[0m )\n\u001b[1;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    572\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    930\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 933\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1231\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1228\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n\u001b[1;32m   1230\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1232\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py:75\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     72\u001b[0m     kwds\u001b[38;5;241m.\u001b[39mpop(key, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     74\u001b[0m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m ensure_dtype_objs(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m---> 75\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m \u001b[43mparsers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:551\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mEmptyDataError\u001b[0m: No columns to parse from file"
     ]
    }
   ],
   "source": [
    "\n",
    "for file in files:\n",
    "    df = load(file, False if file in no_headers else True)\n",
    "    # Encode only numerical columns\n",
    "    for numMethod in numMethods:\n",
    "        print(numMethod)\n",
    "        numcats = df.select_dtypes(include=['float64', 'int64'])\n",
    "        scale_features(df[numcats.columns], method=numMethod)\n",
    "        \n",
    "        # Encode only categorical columns\n",
    "        for ceMethod in ceMethods:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457d5f1f-c076-47c1-86d7-7d43717866f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587bde85-b44f-46b1-8f0d-ad6d215d956c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
