{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fbeb7ec-697d-406a-bc49-75d982115940",
   "metadata": {},
   "source": [
    "# Data Science Homework 4\n",
    "Try different imbalanced classification datasets using $k$-fold cross validation and various classification methods.\n",
    "TODO List:\n",
    "- Make sure we can open all the data as either DataFrame or nparray\n",
    "- Handle categorical data (tokenize, one-hot encoding, ....)\n",
    "- Split each dataset into training and testing dataset.\n",
    "- Perform any necessary sampling, imputaiton, encoding techniques depending on dataset\n",
    "- Perform 5-fold cross-validation to select datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3197ea94-1d37-4113-baf2-de091cac66a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abalone.data', 'arcene_train.data', 'biodeg.csv', 'covtype.data', 'dataset', 'eeg_eye_state.arff', 'heart_2020_cleaned.csv', 'HTRU_2.csv', 'income_evaluation.csv', 'phpAmSP4g.arff', 'Raisin_Dataset.arff', 'spambase.data', 'UCI_Credit_Card.csv', 'WA_Fn-UseC_-Telco-Customer-Churn.csv', 'WineQT.csv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics as metrics\n",
    "import category_encoders as ce # sklearn library\n",
    "import xgboost\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn import preprocessing\n",
    "\n",
    "DATA_DIR = './data'\n",
    "files = os.listdir(DATA_DIR)\n",
    "if '.ipynb_checkpoints' in files:\n",
    "    files.remove('.ipynb_checkpoints')\n",
    "\n",
    "special_delims = { 'arcene_train.data': ' '}\n",
    "no_headers = ['covtype.data', 'arcene_train.data']\n",
    "\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4815b9fb-5923-4120-9df7-0bbf3ffe073a",
   "metadata": {},
   "source": [
    "## Load datasets\n",
    "Some datasets are in csv format, others have just the data. First convert to `DataFrame`s to allow for numeric, categorical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "66cd7606-affa-4a6b-a7de-265d2703dc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load(name, header=True):\n",
    "    sep = special_delims[name] if name in special_delims else ','\n",
    "    name = os.path.join(DATA_DIR, name)\n",
    "    if header:\n",
    "        df = pd.read_csv(name, sep=sep)\n",
    "    else:\n",
    "        df = pd.read_csv(name, header=None, sep=sep)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f6ed6f-5619-4bd8-bd2b-ec3f64277958",
   "metadata": {},
   "source": [
    "## Encode Categorical Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "01a3a02f-649f-45ea-9efc-a9c014157915",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_categorical(data:pd.DataFrame, method='ordinal'):\n",
    "    if method == 'ordinal':\n",
    "        encoder = ce.OrdinalEncoder()\n",
    "    elif method == 'onehot':\n",
    "        encoder = ce.OneHotEncoder()\n",
    "    elif method == 'label':\n",
    "        encoder = preprocessing.LabelEncoder()\n",
    "    return encoder.fit_transform(data)\n",
    "\n",
    "def scale_features(data:pd.DataFrame, method='standard') -> pd.DataFrame:\n",
    "    if method == 'standard':\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "    elif method == 'minmax':\n",
    "        scaler = preprocessing.MinMaxScaler()\n",
    "    return scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5deb65dd-2d12-4851-9783-037c307c731b",
   "metadata": {},
   "source": [
    "## Train model:\n",
    "The evalutation should be fixed on 5-fold cross validation, choose from `RandomForest`, `GBDT`, `XGBoost`,`LightBGM`, `CatBoost`, `KNN`, `Logistic Regression`,`MLP`, `SVM`.\n",
    "\n",
    "Train a new model every iteration of the cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f52818bb-a52f-4e49-90d3-aa1008e90157",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_model(method):\n",
    "    if method == 'xgboost':\n",
    "        model = xgboost.XGBClassifier(5)\n",
    "    elif method == 'knn':\n",
    "        model = KNeighborsClassifier()\n",
    "    elif method == 'forest':\n",
    "        model = RandomForestClassifier(10)\n",
    "    else:\n",
    "        print(f'{method} not supported.')\n",
    "        model = None\n",
    "    return model\n",
    "\n",
    "def get_metric(method):\n",
    "    if method == 'acc':\n",
    "        met = metrics.accuracy_score\n",
    "    elif method == 'auc':\n",
    "        met = metrics.auc\n",
    "    elif method == 'roc_auc':\n",
    "        met = metrics.roc_auc_score\n",
    "    elif method == 'f1':\n",
    "        met = metrics.f1_score\n",
    "    return met\n",
    "\n",
    "def train(x, y, method='xgboost', metric='acc'):\n",
    "    # Perform K-Fold cross validation\n",
    "    metric = get_metric(metric)\n",
    "    n_items = x.shape[0]\n",
    "    kf = KFold( n_splits=5, shuffle=True)\n",
    "    \n",
    "    mean_score = 0 \n",
    "    for i, (train_index, test_index) in enumerate(kf.split(x)):\n",
    "        model = get_model(method)\n",
    "        x_train, x_test = x.iloc[train_index,: ], x.iloc[test_index, :]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        if method == 'xgboost':\n",
    "            y_train -= 1 # Class labels should be zero-based\n",
    "        \n",
    "        model.fit(x_train, y_train)\n",
    "        \n",
    "        # Evaluate\n",
    "        preds = model.predict(x_test)\n",
    "        score = metric(y_test, preds)\n",
    "        mean_score += score\n",
    "        #print(f'\\t{mean_score}')\n",
    "        \n",
    "    mean_score /= (i+1)\n",
    "    #print(f'Mean score over validation: {mean_score} using {method}')\n",
    "\n",
    "    return mean_score\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570aa398-74ca-476c-ab9c-1b703a1cae20",
   "metadata": {},
   "source": [
    "## P1: How does feature scaling (e.g. performing normalization) affect performance?\n",
    "With standardization we apply the formula\n",
    "$$x' = \\frac{x - \\mu}{\\sigma}$$\n",
    "so that we have 0 mean in the training data.\n",
    "\n",
    "For One-hot Encoding we should not use this with tree-based models. For these models we can use label encoding, feature-hashing.\n",
    "We should first encode the categorical values, then train the models on all our datasets both with feature scaling and without."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f9d8aa7e-001d-4e4b-9b41-400240cce3d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on abalone.data\n",
      "train: abalone.data: avg: 0.22168925307280177\n",
      "test: abalone.data: avg: 0.22168925307280177\n",
      "\n",
      "\n",
      "Training on arcene_train.data\n",
      "train: arcene_train.data: avg: 0.02\n",
      "test: arcene_train.data: avg: 0.02\n",
      "\n",
      "\n",
      "Training on biodeg.csv\n",
      "train: biodeg.csv: avg: 0.8511848341232227\n",
      "test: biodeg.csv: avg: 0.8511848341232227\n",
      "\n",
      "\n",
      "Training on covtype.data\n",
      "train: covtype.data: avg: 0.9288052558184697\n",
      "test: covtype.data: avg: 0.9288052558184697\n",
      "\n",
      "\n",
      "Training on dataset\n",
      "train: dataset: avg: 0.8659218065385911\n",
      "test: dataset: avg: 0.8659218065385911\n",
      "\n",
      "\n",
      "Training on eeg_eye_state.arff\n",
      "train: eeg_eye_state.arff: avg: 0.8331775700934578\n",
      "test: eeg_eye_state.arff: avg: 0.8331775700934578\n",
      "\n",
      "\n",
      "Training on heart_2020_cleaned.csv\n",
      "train: heart_2020_cleaned.csv: avg: 0.892831345080442\n",
      "test: heart_2020_cleaned.csv: avg: 0.892831345080442\n",
      "\n",
      "\n",
      "Training on HTRU_2.csv\n",
      "train: HTRU_2.csv: avg: 0.9785997774104374\n",
      "test: HTRU_2.csv: avg: 0.9785997774104374\n",
      "\n",
      "\n",
      "Training on income_evaluation.csv\n",
      "train: income_evaluation.csv: avg: 0.8268170378200319\n",
      "test: income_evaluation.csv: avg: 0.8268170378200319\n",
      "\n",
      "\n",
      "Training on phpAmSP4g.arff\n",
      "train: phpAmSP4g.arff: avg: 0.971883247942866\n",
      "test: phpAmSP4g.arff: avg: 0.971883247942866\n",
      "\n",
      "\n",
      "Training on Raisin_Dataset.arff\n",
      "train: Raisin_Dataset.arff: avg: 0.8555555555555555\n",
      "test: Raisin_Dataset.arff: avg: 0.8555555555555555\n",
      "\n",
      "\n",
      "Training on spambase.data\n",
      "train: spambase.data: avg: 0.9076308832554407\n",
      "test: spambase.data: avg: 0.9076308832554407\n",
      "\n",
      "\n",
      "Training on UCI_Credit_Card.csv\n",
      "train: UCI_Credit_Card.csv: avg: 0.7901\n",
      "test: UCI_Credit_Card.csv: avg: 0.7901\n",
      "\n",
      "\n",
      "Training on WA_Fn-UseC_-Telco-Customer-Churn.csv\n",
      "train: WA_Fn-UseC_-Telco-Customer-Churn.csv: avg: 0.7614666672043358\n",
      "test: WA_Fn-UseC_-Telco-Customer-Churn.csv: avg: 0.7614666672043358\n",
      "\n",
      "\n",
      "Training on WineQT.csv\n",
      "train: WineQT.csv: avg: 0.0\n",
      "test: WineQT.csv: avg: 0.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for file in files:\n",
    "    \n",
    "    df = load(file, False if file in no_headers else True)\n",
    "    # Encode categorical columns\n",
    "    cats = df.select_dtypes(include=['object'])\n",
    "    df[cats.columns] = encode_categorical(cats)\n",
    "    \n",
    "    control = df.copy(deep=True) # Compare with standardized dataset\n",
    "\n",
    "    df, labels = df.iloc[:, :-1], df.iloc[:, -1]\n",
    "    control, c_labels = control.iloc[:, :-1], control.iloc[:, -1]\n",
    "    \n",
    "    # Feature scaling\n",
    "    df = pd.DataFrame(scale_features(df))\n",
    "    \n",
    "    # Train model\n",
    "    print(f'Training on {file}')\n",
    "    score = train(df, labels, method='knn', metric='acc')\n",
    "    print(f'train: {file}: avg: {score}')\n",
    "   \n",
    "    train(control, c_labels, method='knn', metric='acc')\n",
    "    print(f'test: {file}: avg: {score}')\n",
    "    print('\\n')\n",
    "    # The performance is mostly the same across models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13aad0b2",
   "metadata": {},
   "source": [
    "## P2 When using tree-based algorithms, will usng one-hot encoding for categorical features generate worse performance than using label encoding? Why?\n",
    "One-hot encoding transforms inputs with a range of $(1,\\dots,n)$, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d1bc1a45-e699-414f-8259-00d78398f75c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'categories'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [68]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m cats \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mselect_dtypes(include\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      6\u001b[0m one_hot_cols \u001b[38;5;241m=\u001b[39m  encode_categorical(cats, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124monehot\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mone_hot_cols\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcategories\u001b[49m)\n\u001b[0;32m      8\u001b[0m one_hot \u001b[38;5;241m=\u001b[39m df\n\u001b[0;32m     10\u001b[0m cats \u001b[38;5;241m=\u001b[39m label\u001b[38;5;241m.\u001b[39mselect_dtypes(include\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py:5575\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   5569\u001b[0m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set\n\u001b[0;32m   5570\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[0;32m   5571\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessors\n\u001b[0;32m   5572\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[0;32m   5573\u001b[0m ):\n\u001b[0;32m   5574\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[1;32m-> 5575\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'categories'"
     ]
    }
   ],
   "source": [
    "for file in files:\n",
    "    df = load(file, False if file in no_headers else True)\n",
    "    label = df.copy(deep=True) # Use label encoding\n",
    "    \n",
    "    cats = df.select_dtypes(include=['object'])\n",
    "    one_hot_cols =  encode_categorical(cats, method='onehot')\n",
    "    print(one_hot_cols.categories)\n",
    "    one_hot = df\n",
    "    \n",
    "    cats = label.select_dtypes(include=['object'])\n",
    "    label[cats.columns] = encode_categorical(cats, method='label')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9958ab8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
