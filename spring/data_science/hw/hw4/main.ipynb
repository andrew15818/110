{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fbeb7ec-697d-406a-bc49-75d982115940",
   "metadata": {},
   "source": [
    "# Data Science Homework 4\n",
    "Try different imbalanced classification datasets using $k$-fold cross validation and various classification methods.\n",
    "TODO List:\n",
    "- Make sure we can open all the data as either DataFrame or nparray\n",
    "- Handle categorical data (tokenize, one-hot encoding, ....)\n",
    "- Split each dataset into training and testing dataset.\n",
    "- Perform any necessary sampling, imputaiton, encoding techniques depending on dataset\n",
    "- Perform 5-fold cross-validation to select datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e10d493c-9513-4d05-a216-63e80d4ab3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dataset', 'biodeg.csv', 'FuelConsumption.csv', 'Travel_insurance.csv', 'ECommerce.csv', 'heart_2020_cleaned.csv', 'tae.data', 'diabetes_data_upload.csv', 'WA_Fn-UseC_-Telco-Customer-Churn.csv', 'loan.csv', 'online_shoppers_intention.csv', 'income_evaluation.csv', 'abalone.data', 'IBM.csv', 'Churn_Modelling.csv']\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import imblearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost\n",
    "import lightgbm as lgb\n",
    "import neuralnet # My simple nn\n",
    "import sklearn.svm as svm\n",
    "import sklearn.metrics as metrics\n",
    "import category_encoders as ce # sklearn library\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.model_selection import train_test_split,  StratifiedKFold, cross_val_score\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "DATA_DIR = './data'\n",
    "files = os.listdir(DATA_DIR)\n",
    "if '.ipynb_checkpoints' in files:\n",
    "    files.remove('.ipynb_checkpoints')\n",
    "print(files)\n",
    "special_delims = { 'arcene_train.data': ' '}\n",
    "no_headers = ['covtype.data', 'arcene_train.data']\n",
    "\n",
    "numMethods = ['clip',\n",
    "              'standard',\n",
    "              'minmax',\n",
    "              'bin',\n",
    "              'rank'\n",
    "            ]\n",
    "ceMethods = ['onehot',\n",
    "              'label',\n",
    "              'feature',\n",
    "              'target',\n",
    "              'leaveoneout',\n",
    "              'frequency'\n",
    "             ]\n",
    "modelNames = ['forest',\n",
    "             'xgboost',\n",
    "             'lightgbm',\n",
    "             'mlp',\n",
    "             'svm'\n",
    "            ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4815b9fb-5923-4120-9df7-0bbf3ffe073a",
   "metadata": {},
   "source": [
    "## Load datasets\n",
    "Some datasets are in csv format, others have just the data. First convert to `DataFrame`s to allow for numeric, categorical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66cd7606-affa-4a6b-a7de-265d2703dc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load(name, header=True):\n",
    "    try:\n",
    "        sep = special_delims[name] if name in special_delims else ','\n",
    "        name = os.path.join(DATA_DIR, name)\n",
    "        if header:\n",
    "            df = pd.read_csv(name, sep=sep)\n",
    "        else:\n",
    "            df = pd.read_csv(name, header=None, sep=sep)\n",
    "\n",
    "        # Move target column to last\n",
    "        if  'WineQT.csv' in name:\n",
    "            cols = list(df.columns)\n",
    "            x, y = cols.index('quality'), cols.index('Id')\n",
    "            cols[x], cols[y] = cols[y], cols[x]\n",
    "            df = df[cols]\n",
    "        return df\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f6ed6f-5619-4bd8-bd2b-ec3f64277958",
   "metadata": {},
   "source": [
    "## Encode Categorical Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01a3a02f-649f-45ea-9efc-a9c014157915",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_per_column(data, encoder):\n",
    "    for column in data.columns:\n",
    "        data.loc[:, column] = encoder.fit_transform(data.loc[:, column])\n",
    "    return data\n",
    "        \n",
    "def encode_categorical(data:pd.DataFrame, method='ordinal', y=None):\n",
    "    if method == 'ordinal':\n",
    "        encoder = ce.OrdinalEncoder()\n",
    "    elif method == 'onehot':\n",
    "        encoder = ce.OneHotEncoder()\n",
    "    elif method == 'label':\n",
    "        encoder = preprocessing.LabelEncoder()\n",
    "        return encode_per_column(data,encoder)\n",
    "    elif method == 'feature':\n",
    "        encoder = FeatureHasher(n_features=10, input_type='string')\n",
    "        encoder.transform(data.type)\n",
    "    elif method == 'target':\n",
    "        encoder = ce.target_encoder.TargetEncoder()\n",
    "        \n",
    "        return encoder.fit_transform(data.iloc[:, :-1], y)\n",
    "    elif method == 'leaveoneout':\n",
    "        y = data.iloc[:,-1]\n",
    "        encoder = ce.LeaveOneOutEncoder()\n",
    "        return encoder.fit_transform(data.iloc[:,:-1], y)\n",
    "    elif method == 'frequency':\n",
    "        encoder = ce.CountEncoder()\n",
    "    return encoder.fit_transform(data)\n",
    "\n",
    "def scale_features(data:pd.DataFrame, method='standard') -> pd.DataFrame:\n",
    "    if method == 'standard':\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "    elif method == 'minmax':\n",
    "        scaler = preprocessing.MinMaxScaler()\n",
    "    elif method == 'clip':\n",
    "        for column in data.columns:\n",
    "            p01 = data[column].quantile(.01)\n",
    "            p99 = data[column].quantile(.99)\n",
    "            data[column].clip(p01, p99)\n",
    "        return data\n",
    "    elif method == 'bin':\n",
    "        for column in data.columns:\n",
    "            data.loc[:, column] = pd.cut(data.loc[:, column], 5, labels=False, duplicates='drop')\n",
    "        return data\n",
    "    elif method == 'rank':\n",
    "        for column in data.columns:\n",
    "            data.loc[:, column] = data[column].rank()\n",
    "        return data\n",
    "    \n",
    "    return scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5deb65dd-2d12-4851-9783-037c307c731b",
   "metadata": {},
   "source": [
    "## Train model:\n",
    "The evalutation should be fixed on 5-fold cross validation, choose from `RandomForest`, `GBDT`, `XGBoost`,`LightBGM`, `CatBoost`, `KNN`, `Logistic Regression`,`MLP`, `SVM`.\n",
    "\n",
    "Train a new model every iteration of the cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f52818bb-a52f-4e49-90d3-aa1008e90157",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_model(method):\n",
    "    if method == 'xgboost':\n",
    "        model = xgboost.XGBClassifier(5)\n",
    "    elif method == 'knn':\n",
    "        model = KNeighborsClassifier(n_neighbors=3)\n",
    "    elif method == 'forest':\n",
    "        model = RandomForestClassifier(10)\n",
    "    elif method == 'lightgbm':\n",
    "        model = lgb\n",
    "    elif method == 'mlp':\n",
    "        model = neuralnet.tabularNet()\n",
    "    elif method == 'svm':\n",
    "        model = svm.SVC()\n",
    "    else:\n",
    "        print(f'{method} not supported.')\n",
    "        model = None\n",
    "    return model\n",
    "\n",
    "def get_metric(method):\n",
    "    if method == 'acc':\n",
    "        met = metrics.accuracy_score\n",
    "    elif method == 'auc':\n",
    "        met = metrics.auc\n",
    "    elif method == 'roc_auc':\n",
    "        met = metrics.roc_auc_score\n",
    "    elif method == 'f1':\n",
    "        met = metrics.f1_score\n",
    "    elif method == 'prc':\n",
    "        met = metrics.precision_recall_f1_support\n",
    "    return met\n",
    "\n",
    "def train(x, y, method='xgboost', metric='acc'):\n",
    "    # Perform K-Fold cross validation\n",
    "    metric = get_metric(metric)\n",
    "    n_items = x.shape[0]\n",
    "    kf = StratifiedKFold( n_splits=5, shuffle=True)\n",
    "    model = get_model(method)\n",
    "    mean_score = cross_val_score(model, x, y, cv=kf, n_jobs=1)\n",
    "    '''\n",
    "    mean_score = 0 \n",
    "    for i, (train_index, test_index) in enumerate(kf.split(x, y)):\n",
    "        model = get_model(method)\n",
    "        x_train, x_test = x.iloc[train_index,: ], x.iloc[test_index, :]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # \n",
    "        try:\n",
    "            if y_test.min().item()  != 0 and method == 'xgboost':\n",
    "                y_train -= y_train.min().item() # Class labels should be zero-based\n",
    "        except ValueError:\n",
    "            pass\n",
    "            \n",
    "        model.fit(x_train, y_train)\n",
    "        \n",
    "        # Evaluate\n",
    "        preds = model.predict(x_test)\n",
    "        score = metric(y_test, preds)\n",
    "        mean_score += score\n",
    "        \n",
    "    mean_score /= (i+1)\n",
    "    #print(f'Mean score over validation: {mean_score} using {method}')\n",
    "    '''\n",
    "    return mean_score.mean(axis=0)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570aa398-74ca-476c-ab9c-1b703a1cae20",
   "metadata": {},
   "source": [
    "## P1: How does feature scaling (e.g. performing normalization) affect performance?\n",
    "With standardization we apply the formula\n",
    "$$x' = \\frac{x - \\mu}{\\sigma}$$\n",
    "so that we have 0 mean in the training data.\n",
    "\n",
    "For One-hot Encoding we should not use this with tree-based models. For these models we can use label encoding, feature-hashing.\n",
    "We should first encode the categorical values, then train the models on all our datasets both with feature scaling and without."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f9d8aa7e-001d-4e4b-9b41-400240cce3d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on dataset\n",
      "train: dataset: avg: 0.8862436804853389\n",
      "test: dataset: avg: 0.8786872261543646\n",
      "\n",
      "\n",
      "Training on biodeg.csv\n",
      "train: biodeg.csv: avg: 0.8663507109004739\n",
      "test: biodeg.csv: avg: 0.8559241706161138\n",
      "\n",
      "\n",
      "Training on FuelConsumption.csv\n",
      "train: FuelConsumption.csv: avg: 0.8002450570871623\n",
      "test: FuelConsumption.csv: avg: 0.7696017822333612\n",
      "\n",
      "\n",
      "Training on Travel_insurance.csv\n",
      "train: Travel_insurance.csv: avg: 0.8067313899472172\n",
      "test: Travel_insurance.csv: avg: 0.8072503575813578\n",
      "\n",
      "\n",
      "Training on ECommerce.csv\n",
      "train: ECommerce.csv: avg: 0.6626972177436025\n",
      "test: ECommerce.csv: avg: 0.6667863078258713\n",
      "\n",
      "\n",
      "Training on heart_2020_cleaned.csv\n",
      "train: heart_2020_cleaned.csv: avg: 0.8794196281993152\n",
      "test: heart_2020_cleaned.csv: avg: 0.8792976750730936\n",
      "\n",
      "\n",
      "Training on tae.data\n",
      "train: tae.data: avg: 0.6133333333333334\n",
      "test: tae.data: avg: 0.6199999999999999\n",
      "\n",
      "\n",
      "Training on diabetes_data_upload.csv\n",
      "train: diabetes_data_upload.csv: avg: 0.9846153846153847\n",
      "test: diabetes_data_upload.csv: avg: 0.976923076923077\n",
      "\n",
      "\n",
      "Training on WA_Fn-UseC_-Telco-Customer-Churn.csv\n",
      "train: WA_Fn-UseC_-Telco-Customer-Churn.csv: avg: 0.7837580247112717\n",
      "test: WA_Fn-UseC_-Telco-Customer-Churn.csv: avg: 0.7881573004709981\n",
      "\n",
      "\n",
      "Training on loan.csv\n",
      "train: loan.csv: avg: 0.8121739130434783\n",
      "test: loan.csv: avg: 0.8208281573498966\n",
      "\n",
      "\n",
      "Training on online_shoppers_intention.csv\n",
      "train: online_shoppers_intention.csv: avg: 0.8961881589618816\n",
      "test: online_shoppers_intention.csv: avg: 0.8936739659367398\n",
      "\n",
      "\n",
      "Training on income_evaluation.csv\n",
      "train: income_evaluation.csv: avg: 0.8485917552534319\n",
      "test: income_evaluation.csv: avg: 0.8517859008876971\n",
      "\n",
      "\n",
      "Training on abalone.data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/poncedeleon/.local/lib/python3.10/site-packages/sklearn/model_selection/_split.py:676: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: abalone.data: avg: 0.2339010973268773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/poncedeleon/.local/lib/python3.10/site-packages/sklearn/model_selection/_split.py:676: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: abalone.data: avg: 0.23317279316964154\n",
      "\n",
      "\n",
      "Training on IBM.csv\n",
      "train: IBM.csv: avg: 0.15918367346938775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/poncedeleon/.local/lib/python3.10/site-packages/sklearn/model_selection/_split.py:676: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/home/poncedeleon/.local/lib/python3.10/site-packages/sklearn/model_selection/_split.py:676: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: IBM.csv: avg: 0.14217687074829932\n",
      "\n",
      "\n",
      "Training on Churn_Modelling.csv\n",
      "train: Churn_Modelling.csv: avg: 0.8506\n",
      "test: Churn_Modelling.csv: avg: 0.8484\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for file in files:\n",
    "    print(f'Training on {file}')\n",
    "   \n",
    "    df = load(file, False if file in no_headers else True)\n",
    "    # Encode categorical columns\n",
    "    cats = df.select_dtypes(include=['object'])\n",
    "    df = encode_categorical(df)\n",
    "    \n",
    "    control = df.copy(deep=True) # Compare with standardized dataset\n",
    "\n",
    "    df, labels = df.iloc[:, :-1], df.iloc[:, -1]\n",
    "    control, c_labels = control.iloc[:, :-1], control.iloc[:, -1]\n",
    "    \n",
    "    # Feature scaling\n",
    "    df = pd.DataFrame(scale_features(df))\n",
    "    \n",
    "    if file == 'parkinsons.csv':\n",
    "        continue\n",
    "        # Train model\n",
    "    score = train(df, labels, method='forest', metric='acc')\n",
    "    print(f'train: {file}: avg: {score}')\n",
    "   \n",
    "    score = train(control, c_labels, method='forest', metric='acc')\n",
    "    print(f'test: {file}: avg: {score}')\n",
    "    print('\\n')\n",
    "    # The performance is mostly the same across models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13aad0b2",
   "metadata": {},
   "source": [
    "## P2 When using tree-based algorithms, will usng one-hot encoding for categorical features generate worse performance than using label encoding? Why?\n",
    "One-hot encoding transforms inputs with a range of $(1,\\dots,n)$. This method generates extra columns and usually results in a sparse matrix.\n",
    "\n",
    "Label Encoding adds replaces each unique class name with an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bc1a45-e699-414f-8259-00d78398f75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on dataset\n",
      "\tOnehot avg: 0.8607162116616112\n",
      "\tlabel avg: 0.8786990225817325\n",
      "\n",
      "\n",
      "Training on biodeg.csv\n",
      "\tOnehot avg: 0.8161137440758294\n",
      "\tlabel avg: 0.8597156398104266\n",
      "\n",
      "\n",
      "Training on FuelConsumption.csv\n",
      "\tOnehot avg: 0.5676524644945699\n",
      "\tlabel avg: 0.7832414369256474\n",
      "\n",
      "\n",
      "Training on Travel_insurance.csv\n",
      "\tOnehot avg: 0.7453362530536815\n",
      "\tlabel avg: 0.8022227004037822\n",
      "\n",
      "\n",
      "Training on ECommerce.csv\n",
      "\tOnehot avg: 0.6575160196783663\n",
      "\tlabel avg: 0.6616965149448095\n",
      "\n",
      "\n",
      "Training on heart_2020_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "for file in files:\n",
    "    df = load(file, False if file in no_headers else True)\n",
    "    target_column = df.columns[-1]\n",
    "    label = df.copy(deep=True) # Use label encoding\n",
    "    \n",
    "    cats = label.select_dtypes(include=['object'])\n",
    "\n",
    "    # Ignore files w/ no categorical data, otherw encoder error\n",
    "    if cats.empty:\n",
    "        print(f'No categorical in {file}\\n')\n",
    "        continue\n",
    "        \n",
    "    one_hot = pd.get_dummies(df.iloc[:, :-1], sparse=False)\n",
    "    cats = label.select_dtypes(include=['object'])\n",
    "\n",
    "    # Apply Label Encoder to each column\n",
    "\n",
    "    label.loc[:, cats.columns] = encode_categorical(label.loc[:, cats.columns], method='label')\n",
    "    \n",
    "    # If target column is not categorical, labels will only be one column\n",
    "    if df.dtypes[target_column] == object:\n",
    "        n_unique = df[target_column].unique().shape[0] - 1\n",
    "    else:\n",
    "        n_unique = 1\n",
    "    \n",
    "    x_ohe, y_ohe = one_hot, df[target_column]\n",
    "    x_label, y_label = label.iloc[:,:-1], label.iloc[:,-1]\n",
    "    \n",
    "    #print(f'{file}: xcols: {len(x_label)} ycols: {len(y_label)}')\n",
    "    #print(one_hot)\n",
    "    print(f'Training on {file}')\n",
    "\n",
    "    score = train(x_ohe, y_ohe, method='knn', metric='acc')\n",
    "    print(f'\\tOnehot avg: {score}')\n",
    "    score = train(x_label, y_label, method='forest', metric='acc')\n",
    "    print(f'\\tlabel avg: {score}')\n",
    "    print('\\n')\n",
    "    ## There is a slight difference in performance (label encoder usually better by a bit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9237acb9-d3f7-46f9-8114-72f6010104c9",
   "metadata": {},
   "source": [
    "## P3\n",
    "Will feature binning provide performance improvement? When is binning useful (which models or which kinds of datasets)?\n",
    "\n",
    "Loop through each column in the dataframe and bin individually.\n",
    "We use LabelEncoder for categorical features on both DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d82bdc0-2b4e-46ab-9172-1221cf1df7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "### for file in files:\n",
    "    df = load(file, False if file in no_headers else True)\n",
    "    binned = df.copy(deep=True)\n",
    "\n",
    "    # Bin each column individually, use LabelEncoder for categorical\n",
    "    for column in binned.columns:\n",
    "        if (binned[column].dtype.name == 'int64'\n",
    "           or binned[column].dtype.name == 'float64'):\n",
    "            #n_unique = binned[column].unique() \n",
    "            binned[column]= pd.cut(binned[column], 5, labels=False, duplicates='drop')\n",
    "            #binned[column].fillna(0, inplace=True)\n",
    "\n",
    "            if binned[column].isnull().values.any():\n",
    "                binned[colum] = 0\n",
    "        #elif check if column is categorical\n",
    "        elif binned[column].dtype.name == 'object':\n",
    "            binned[column] = encode_categorical(binned[column], method='label')\n",
    "            df[column] = encode_categorical(df[column], method='ordinal')\n",
    "\n",
    "    # Train\n",
    "    x_bin, y_bin = binned.iloc[:, :-1], binned.iloc[:, -1]\n",
    "    x_label, y_label = df.iloc[:, :-1], df.iloc[:, -1]\n",
    "    \n",
    "    print(f'Training on {file}')\n",
    "    score = train(x_bin, y_bin, method='forest', metric='acc')\n",
    "    print(f'train: {file}: avg: {score}')\n",
    "    \n",
    "    score = train(x_label, y_label, method='forest', metric='acc')\n",
    "    print(f'test: {file}: avg: {score}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2e941f-c7b6-49e2-9f0b-db9271a52a80",
   "metadata": {},
   "source": [
    "## P4\n",
    "Compare the performance of 6 different categorical feature encoding methods based on Random Forest, XGBoost LightGBM, MLP, SVM. Which of the 6 encoding methods is better?\n",
    "TODO: Finish implementing neural network (missing dataloader)\n",
    "\n",
    "Models to test: \n",
    "1. One hot encoding\n",
    "2. Label Encoding\n",
    "3. Feature Hasing\n",
    "4. Frequency Encoding\n",
    "5. Target Encoding\n",
    "6. Leave One Out Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a8671b37-af75-4c5f-850e-b0194e58641d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/poncedeleon/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/poncedeleon/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/poncedeleon/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/poncedeleon/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/poncedeleon/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/poncedeleon/.local/lib/python3.10/site-packages/xgboost/core.py:525: FutureWarning: Pass `objective` as keyword args.  Passing these as positional arguments will be considered as error in future releases.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t(onehot, forest): 0.88567071115605\n",
      "\t(onehot, xgboost): nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/poncedeleon/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "5 fits failed out of a total of 5.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/poncedeleon/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/poncedeleon/.local/lib/python3.10/site-packages/xgboost/core.py\", line 532, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/home/poncedeleon/.local/lib/python3.10/site-packages/xgboost/sklearn.py\", line 1400, in fit\n",
      "    self._Booster = train(\n",
      "  File \"/home/poncedeleon/.local/lib/python3.10/site-packages/xgboost/core.py\", line 532, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/home/poncedeleon/.local/lib/python3.10/site-packages/xgboost/training.py\", line 181, in train\n",
      "    bst.update(dtrain, i, obj)\n",
      "  File \"/home/poncedeleon/.local/lib/python3.10/site-packages/xgboost/core.py\", line 1733, in update\n",
      "    _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n",
      "  File \"/home/poncedeleon/.local/lib/python3.10/site-packages/xgboost/core.py\", line 203, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: [22:17:33] ../src/objective/objective.cc:26: Unknown objective function: `5`\n",
      "Objective candidate: survival:aft\n",
      "Objective candidate: binary:hinge\n",
      "Objective candidate: multi:softmax\n",
      "Objective candidate: multi:softprob\n",
      "Objective candidate: rank:pairwise\n",
      "Objective candidate: rank:ndcg\n",
      "Objective candidate: rank:map\n",
      "Objective candidate: reg:squarederror\n",
      "Objective candidate: reg:squaredlogerror\n",
      "Objective candidate: reg:logistic\n",
      "Objective candidate: binary:logistic\n",
      "Objective candidate: binary:logitraw\n",
      "Objective candidate: reg:linear\n",
      "Objective candidate: reg:pseudohubererror\n",
      "Objective candidate: count:poisson\n",
      "Objective candidate: survival:cox\n",
      "Objective candidate: reg:gamma\n",
      "Objective candidate: reg:tweedie\n",
      "\n",
      "Stack trace:\n",
      "  [bt] (0) /home/poncedeleon/.local/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x283dad) [0x7f5a46c83dad]\n",
      "  [bt] (1) /home/poncedeleon/.local/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x284409) [0x7f5a46c84409]\n",
      "  [bt] (2) /home/poncedeleon/.local/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x20d0f2) [0x7f5a46c0d0f2]\n",
      "  [bt] (3) /home/poncedeleon/.local/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x214f7d) [0x7f5a46c14f7d]\n",
      "  [bt] (4) /home/poncedeleon/.local/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x20f8c9) [0x7f5a46c0f8c9]\n",
      "  [bt] (5) /home/poncedeleon/.local/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x68) [0x7f5a46aa9758]\n",
      "  [bt] (6) /usr/lib/libffi.so.8(+0x7536) [0x7f5a9d521536]\n",
      "  [bt] (7) /usr/lib/libffi.so.8(+0x4037) [0x7f5a9d51e037]\n",
      "  [bt] (8) /usr/lib/python3.10/lib-dynload/_ctypes.cpython-310-x86_64-linux-gnu.so(+0x7e9e) [0x7f5a9d3dde9e]\n",
      "\n",
      "\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "estimator should be an estimator implementing 'fit' method, <module 'lightgbm' from '/home/poncedeleon/.local/lib/python3.10/site-packages/lightgbm/__init__.py'> was passed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [40]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m labelIndex \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     17\u001b[0m enc_train, enc_test \u001b[38;5;241m=\u001b[39m encoded\u001b[38;5;241m.\u001b[39miloc[:,:\u001b[38;5;241m-\u001b[39mlabelIndex], encoded\u001b[38;5;241m.\u001b[39miloc[:,\u001b[38;5;241m-\u001b[39mlabelIndex:]\n\u001b[0;32m---> 18\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43menc_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodelName\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodelName\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscore\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[0;32mIn [36]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(x, y, method, metric)\u001b[0m\n\u001b[1;32m     36\u001b[0m kf \u001b[38;5;241m=\u001b[39m StratifiedKFold( n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     37\u001b[0m model \u001b[38;5;241m=\u001b[39m get_model(method)\n\u001b[0;32m---> 38\u001b[0m mean_score \u001b[38;5;241m=\u001b[39m \u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03mmean_score = 0 \u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;124;03mfor i, (train_index, test_index) in enumerate(kf.split(x, y)):\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;124;03m#print(f'Mean score over validation: {mean_score} using {method}')\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mean_score\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:507\u001b[0m, in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;124;03m\"\"\"Evaluate a score by cross-validation.\u001b[39;00m\n\u001b[1;32m    390\u001b[0m \n\u001b[1;32m    391\u001b[0m \u001b[38;5;124;03mRead more in the :ref:`User Guide <cross_validation>`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;124;03m    loss function.\u001b[39;00m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[0;32m--> 507\u001b[0m scorer \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_scoring\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscoring\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    509\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m cross_validate(\n\u001b[1;32m    510\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[1;32m    511\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m     error_score\u001b[38;5;241m=\u001b[39merror_score,\n\u001b[1;32m    521\u001b[0m )\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py:448\u001b[0m, in \u001b[0;36mcheck_scoring\u001b[0;34m(estimator, scoring, allow_none)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;124;03m\"\"\"Determine scorer from user options.\u001b[39;00m\n\u001b[1;32m    423\u001b[0m \n\u001b[1;32m    424\u001b[0m \u001b[38;5;124;03mA TypeError will be thrown if the estimator cannot be scored.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;124;03m    ``scorer(estimator, X, y)``.\u001b[39;00m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(estimator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 448\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    449\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mestimator should be an estimator implementing \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m method, \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m was passed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    450\u001b[0m         \u001b[38;5;241m%\u001b[39m estimator\n\u001b[1;32m    451\u001b[0m     )\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(scoring, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m get_scorer(scoring)\n",
      "\u001b[0;31mTypeError\u001b[0m: estimator should be an estimator implementing 'fit' method, <module 'lightgbm' from '/home/poncedeleon/.local/lib/python3.10/site-packages/lightgbm/__init__.py'> was passed"
     ]
    }
   ],
   "source": [
    "for file in files:\n",
    "    print(f'Checking {file}')\n",
    "    df = load(file, header = False if file in no_headers else True)\n",
    "    cats = df.select_dtypes(include=['object'])\n",
    "    for method in ceMethods:\n",
    "        # Encode features\n",
    "        encoded = df.copy()\n",
    "        \n",
    "    \n",
    "        encoded = encode_categorical(encoded, method=method)\n",
    "     \n",
    "            \n",
    "        for modelName in modelNames:\n",
    "            # Number of class columns, necessary for OHE\n",
    "            #labelIndex = len(df.iloc[:,-1].unique()) if method == 'onehot' else 1\n",
    "            labelIndex = 1\n",
    "            enc_train, enc_test = encoded.iloc[:,:-labelIndex], encoded.iloc[:,-labelIndex:]\n",
    "            score = train(enc_train, enc_test, modelName)\n",
    "            print(f'\\t({method}, {modelName}): {score}')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d41521-2aa7-4487-afb4-05ab54bc0265",
   "metadata": {},
   "source": [
    "## P5\n",
    "Which combinations of numerical and categorical feature transformation methods generally lead to better results?\n",
    "\n",
    "Here we can test the different numerical and categorical methods on each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "41309118-82c9-45a7-901d-f5486a76c1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking: dataset\n",
      "\tNumerical: clip, Categorical: onehot, Acc: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/poncedeleon/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "5 fits failed out of a total of 5.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/poncedeleon/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/poncedeleon/.local/lib/python3.10/site-packages/sklearn/ensemble/_forest.py\", line 327, in fit\n",
      "    X, y = self._validate_data(\n",
      "  File \"/home/poncedeleon/.local/lib/python3.10/site-packages/sklearn/base.py\", line 581, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"/home/poncedeleon/.local/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 964, in check_X_y\n",
      "    X = check_array(\n",
      "  File \"/home/poncedeleon/.local/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 746, in check_array\n",
      "    array = np.asarray(array, order=order, dtype=dtype)\n",
      "  File \"/home/poncedeleon/.local/lib/python3.10/site-packages/pandas/core/generic.py\", line 2072, in __array__\n",
      "    return np.asarray(self._values, dtype=dtype)\n",
      "ValueError: could not convert string to float: 'male'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "y should be a 1d array, got an array of shape (1723, 4) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [41]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ceMethod \u001b[38;5;129;01min\u001b[39;00m ceMethods:\n\u001b[1;32m     11\u001b[0m     cats \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mselect_dtypes(include\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 12\u001b[0m     \u001b[43mencode_categorical\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcats\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mceMethod\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39miloc[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], df\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     14\u001b[0m     score \u001b[38;5;241m=\u001b[39m train(x, y, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforest\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[0;32mIn [35]\u001b[0m, in \u001b[0;36mencode_categorical\u001b[0;34m(data, method, y)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfrequency\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     26\u001b[0m     encoder \u001b[38;5;241m=\u001b[39m ce\u001b[38;5;241m.\u001b[39mCountEncoder()\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/preprocessing/_label.py:115\u001b[0m, in \u001b[0;36mLabelEncoder.fit_transform\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, y):\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;124;03m\"\"\"Fit label encoder and return encoded labels.\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m        Encoded labels.\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcolumn_or_1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_, y \u001b[38;5;241m=\u001b[39m _unique(y, return_inverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:1038\u001b[0m, in \u001b[0;36mcolumn_or_1d\u001b[0;34m(y, warn)\u001b[0m\n\u001b[1;32m   1029\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1030\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA column-vector y was passed when a 1d array was\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1031\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m expected. Please change the shape of y to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m   1035\u001b[0m         )\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mravel(y)\n\u001b[0;32m-> 1038\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my should be a 1d array, got an array of shape \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(shape)\n\u001b[1;32m   1040\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: y should be a 1d array, got an array of shape (1723, 4) instead."
     ]
    }
   ],
   "source": [
    "\n",
    "for file in files:\n",
    "    df = load(file, False if file in no_headers else True)\n",
    "    print(f'Checking: {file}')\n",
    "    # Encode only numerical columns\n",
    "    for numMethod in numMethods:\n",
    "        numcats = df.select_dtypes(include=['float64', 'int64'])\n",
    "        scale_features(df[numcats.columns], method=numMethod)\n",
    "        \n",
    "        # Encode only categorical columns\n",
    "        for ceMethod in ceMethods:\n",
    "            cats = df.select_dtypes(include=['object'])\n",
    "            encode_categorical(df[cats.columns], method=ceMethod)\n",
    "            x, y = df.iloc[:, :-1], df.iloc[:, -1]\n",
    "            score = train(x, y, method='forest')\n",
    "            print(f'\\tNumerical: {numMethod}, Categorical: {ceMethod}, Acc: {score}')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e9c9e8-0c2d-4455-b794-ede932074f35",
   "metadata": {},
   "source": [
    "## P6\n",
    "If the number of possible categorical values of a feature is high, which encoding method among target encoding, one-hot encoding, and label encoding will have better performance? Why?\n",
    "\n",
    "Go through each file, test each encoding method, and measure the $n$ datasets with most unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "457d5f1f-c076-47c1-86d7-7d43717866f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on dataset\n",
      "2\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Supported target types are: ('binary', 'multiclass'). Got 'multilabel-indicator' instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/parallel.py:822\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 822\u001b[0m     tasks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ready_batches\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m queue\u001b[38;5;241m.\u001b[39mEmpty:\n\u001b[1;32m    824\u001b[0m     \u001b[38;5;66;03m# slice the iterator n_jobs * batchsize items at a time. If the\u001b[39;00m\n\u001b[1;32m    825\u001b[0m     \u001b[38;5;66;03m# slice returns less than that, then the current batchsize puts\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;66;03m# accordingly to distribute evenly the last items between all\u001b[39;00m\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;66;03m# workers.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/queue.py:168\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qsize():\n\u001b[0;32m--> 168\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mEmpty\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [42]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m x, y \u001b[38;5;241m=\u001b[39m df_copy\u001b[38;5;241m.\u001b[39miloc[:,:\u001b[38;5;241m-\u001b[39mn_labels], df_copy\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m-\u001b[39mn_labels:]\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mmethod: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menc_method\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,  max unique categories: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_uniques\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscore\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[0;32mIn [36]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(x, y, method, metric)\u001b[0m\n\u001b[1;32m     36\u001b[0m kf \u001b[38;5;241m=\u001b[39m StratifiedKFold( n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     37\u001b[0m model \u001b[38;5;241m=\u001b[39m get_model(method)\n\u001b[0;32m---> 38\u001b[0m mean_score \u001b[38;5;241m=\u001b[39m \u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03mmean_score = 0 \u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;124;03mfor i, (train_index, test_index) in enumerate(kf.split(x, y)):\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;124;03m#print(f'Mean score over validation: {mean_score} using {method}')\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mean_score\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:509\u001b[0m, in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[1;32m    507\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[0;32m--> 509\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:267\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[1;32m    266\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[0;32m--> 267\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscorers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    286\u001b[0m _warn_about_fit_failures(results, error_score)\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/parallel.py:1043\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1035\u001b[0m     \u001b[38;5;66;03m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;66;03m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1040\u001b[0m     \u001b[38;5;66;03m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m     \u001b[38;5;66;03m# remaining jobs.\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 1043\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1044\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1046\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/parallel.py:833\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    830\u001b[0m n_jobs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_effective_n_jobs\n\u001b[1;32m    831\u001b[0m big_batch_size \u001b[38;5;241m=\u001b[39m batch_size \u001b[38;5;241m*\u001b[39m n_jobs\n\u001b[0;32m--> 833\u001b[0m islice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mitertools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mislice\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbig_batch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(islice) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    835\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:267\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[1;32m    266\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[0;32m--> 267\u001b[0m results \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[1;32m    268\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    269\u001b[0m         clone(estimator),\n\u001b[1;32m    270\u001b[0m         X,\n\u001b[1;32m    271\u001b[0m         y,\n\u001b[1;32m    272\u001b[0m         scorers,\n\u001b[1;32m    273\u001b[0m         train,\n\u001b[1;32m    274\u001b[0m         test,\n\u001b[1;32m    275\u001b[0m         verbose,\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    277\u001b[0m         fit_params,\n\u001b[1;32m    278\u001b[0m         return_train_score\u001b[38;5;241m=\u001b[39mreturn_train_score,\n\u001b[1;32m    279\u001b[0m         return_times\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    280\u001b[0m         return_estimator\u001b[38;5;241m=\u001b[39mreturn_estimator,\n\u001b[1;32m    281\u001b[0m         error_score\u001b[38;5;241m=\u001b[39merror_score,\n\u001b[1;32m    282\u001b[0m     )\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m cv\u001b[38;5;241m.\u001b[39msplit(X, y, groups)\n\u001b[1;32m    284\u001b[0m )\n\u001b[1;32m    286\u001b[0m _warn_about_fit_failures(results, error_score)\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/model_selection/_split.py:340\u001b[0m, in \u001b[0;36m_BaseKFold.split\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_splits \u001b[38;5;241m>\u001b[39m n_samples:\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    334\u001b[0m         (\n\u001b[1;32m    335\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot have number of splits n_splits=\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m greater\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    336\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m than the number of samples: n_samples=\u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    337\u001b[0m         )\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_splits, n_samples)\n\u001b[1;32m    338\u001b[0m     )\n\u001b[0;32m--> 340\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39msplit(X, y, groups):\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m train, test\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/model_selection/_split.py:86\u001b[0m, in \u001b[0;36mBaseCrossValidator.split\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m     84\u001b[0m X, y, groups \u001b[38;5;241m=\u001b[39m indexable(X, y, groups)\n\u001b[1;32m     85\u001b[0m indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(_num_samples(X))\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m test_index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_test_masks(X, y, groups):\n\u001b[1;32m     87\u001b[0m     train_index \u001b[38;5;241m=\u001b[39m indices[np\u001b[38;5;241m.\u001b[39mlogical_not(test_index)]\n\u001b[1;32m     88\u001b[0m     test_index \u001b[38;5;241m=\u001b[39m indices[test_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/model_selection/_split.py:709\u001b[0m, in \u001b[0;36mStratifiedKFold._iter_test_masks\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    708\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_iter_test_masks\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, groups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 709\u001b[0m     test_folds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_test_folds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    710\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_splits):\n\u001b[1;32m    711\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m test_folds \u001b[38;5;241m==\u001b[39m i\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/model_selection/_split.py:652\u001b[0m, in \u001b[0;36mStratifiedKFold._make_test_folds\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    650\u001b[0m allowed_target_types \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    651\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m type_of_target_y \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m allowed_target_types:\n\u001b[0;32m--> 652\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    653\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSupported target types are: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    654\u001b[0m             allowed_target_types, type_of_target_y\n\u001b[1;32m    655\u001b[0m         )\n\u001b[1;32m    656\u001b[0m     )\n\u001b[1;32m    658\u001b[0m y \u001b[38;5;241m=\u001b[39m column_or_1d(y)\n\u001b[1;32m    660\u001b[0m _, y_idx, y_inv \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y, return_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_inverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mValueError\u001b[0m: Supported target types are: ('binary', 'multiclass'). Got 'multilabel-indicator' instead."
     ]
    }
   ],
   "source": [
    "enc_methods = [#'target',\n",
    "               'onehot',\n",
    "               'label',\n",
    "              ]\n",
    "\n",
    "for file in files:\n",
    "    df = load(file, header=False if file in no_headers else True)\n",
    "    print(f'Training on {file}')\n",
    "    for enc_method in enc_methods:\n",
    "        df_copy = df.copy()\n",
    "\n",
    "        # OHE produces this many label columns\n",
    "        n_labels = df_copy.iloc[:, -1].nunique() if enc_method == 'onehot' else 1\n",
    "        \n",
    "        # Encode df using method\n",
    "        cats = df_copy.select_dtypes(include=['object'])\n",
    "        if cats.empty:\n",
    "            print(f'{file} has not categorical')\n",
    "            continue\n",
    "            \n",
    "        max_uniques = max([len(df_copy[col].unique()) for col in cats.columns])\n",
    "        if enc_method == 'onehot':\n",
    "            print(n_labels)\n",
    "            df_copy = pd.get_dummies(df_copy)\n",
    "        else:\n",
    "            df_copy[cats.columns] = encode_categorical(df_copy[cats.columns], method=enc_method, y=df_copy.iloc[:,-1])\n",
    "            \n",
    "        x, y = df_copy.iloc[:,:-n_labels], df_copy.iloc[:, -n_labels:]\n",
    "\n",
    "        # Train model\n",
    "        score = train(x, y, method='forest')\n",
    "        print(f'\\tmethod: {enc_method},  max unique categories: {max_uniques} score: {score}')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e41254b-50f4-4a47-a6d1-f90562936026",
   "metadata": {},
   "source": [
    "## Plot class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60816f46-f119-40f6-94d9-400e290ae93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset\n",
      "biodeg.csv\n",
      "FuelConsumption.csv\n",
      "Travel_insurance.csv\n",
      "ECommerce.csv\n",
      "heart_2020_cleaned.csv\n",
      "tae.data\n",
      "diabetes_data_upload.csv\n",
      "WA_Fn-UseC_-Telco-Customer-Churn.csv\n",
      "loan.csv\n",
      "online_shoppers_intention.csv\n",
      "income_evaluation.csv\n",
      "abalone.data\n",
      "IBM.csv\n",
      "Churn_Modelling.csv\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 15 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "   \n",
    "fig, axs = plt.subplots(3, 5)\n",
    "fig.suptitle('Class Distribution in Datasets')\n",
    "row, col = 0, 0\n",
    "fig.tight_layout()\n",
    "for i, file in enumerate(files):\n",
    "    print(f'{file}')\n",
    "    df = load(file, False if file in no_headers else True)\n",
    "    row = row + 1 if i % 3 != 0 else 0\n",
    "    col = col + 1 if i % 5 != 0 else 0\n",
    "    \n",
    "    target = df.iloc[:, -1]\n",
    "    names, counts = [],[]\n",
    "    for unique in target.unique():\n",
    "        count = target[target == unique].count()\n",
    "        counts.append(count)\n",
    "        names.append(unique)\n",
    "    #print(names, counts)\n",
    "    axs[row, col].bar( range(len(counts)),counts) \n",
    "    axs[row,col].set_title(file.split('.')[0][:10])\n",
    "    \n",
    "    #plot_imbalance(df, file)\n",
    "plt.show()\n",
    "fig.savefig('./report/img/distribution.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae5bfd8-d7bb-44ce-98de-c4cc4699ad07",
   "metadata": {},
   "source": [
    "## Imbalanced data samplers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2331ed6d-f5db-49e5-9011-11a4418847f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_imb_sampler(method:str, y=None):\n",
    "    if method == 'nearmiss':\n",
    "        return imblearn.under_sampling.NearMiss(n_neighbors=get_least_frequent(y))\n",
    "    elif method == 'clustercentroids':\n",
    "        return imblearn.under_sampling.ClusterCentroids()\n",
    "    elif method == 'condensednn':\n",
    "        return imblearn.under_sampling.CondensedNearestNeighbour(n_neighbors=get_least_frequent(y))\n",
    "    elif method == 'editednn':\n",
    "        return imblearn.under_sampling.EditedNearestNeighbours(n_neighbors=get_least_frequent(y))\n",
    "    elif method == 'neighbourhoodcleaning':\n",
    "        return imblearn.under_sampling.NeighbourhoodCleaningRule(n_neighbors=get_least_frequent(y))\n",
    "    elif method == 'tomeklinks':\n",
    "        return imblearn.under_sampling.TomekLinks()\n",
    "    elif method == 'onesidedselection':\n",
    "        return imblearn.under_sampling.OneSidedSelection()\n",
    "    elif method == 'smote':\n",
    "        return imblearn.over_sampling.SMOTE()\n",
    "    elif method == 'borderlinesmote':\n",
    "        return imblearn.over_sampling.BorderlineSMOTE()\n",
    "    elif method == 'adasyn':\n",
    "        return imblearn.over_sampling.ADASYN(n_neighbors=get_least_frequent(y))\n",
    "    elif method == 'svmsmote':\n",
    "        return imblearn.over_sampling.SVMSMOTE(k_neighbors=get_least_frequent(y))\n",
    "\n",
    "def get_least_frequent(y:pd.Series):\n",
    "    return min(3, min(y.value_counts()))\n",
    "\n",
    "def get_imb_sample(x:pd.DataFrame, y:pd.Series, s_method:str='nearmiss'):\n",
    "    sampler = get_imb_sampler(us_method, y)\n",
    "    # find least frequent class\n",
    "    \n",
    "    #sampler = sampler(n_neighbors=get_least_frequent(y))\n",
    "    return sampler.fit_resample(x, y)\n",
    "\n",
    "def train_imb(x:pd.DataFrame, y:pd.DataFrame, model='forest', eval_method='prc'): \n",
    "    # TODO: Use OVR classifier to \n",
    "    # Perform K-Fold cross validation'\n",
    "   \n",
    "    n_items = x.shape[0]\n",
    "    kf = StratifiedKFold(n_splits=get_least_frequent(y), shuffle=True)\n",
    "    mean_prec, mean_rec, mean_f, mean_supp = 0, 0, 0, 0\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(x, y)):\n",
    "        model = get_model('forest')\n",
    "        x_train, x_test = x.iloc[train_index,: ], x.iloc[test_index, :]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "            \n",
    "        model.fit(x_train, y_train)\n",
    "        \n",
    "        # Evaluate\n",
    "        preds = model.predict(x_test)\n",
    "        prec,rec,f,supp =  metrics.precision_recall_fscore_support(y_test, preds, zero_division=0)\n",
    "        mean_prec += prec\n",
    "        mean_rec += rec\n",
    "        mean_f += f\n",
    "        mean_supp += supp\n",
    "    mean_prec = mean_prec.mean()\n",
    "    mean_rec = mean_rec.mean()\n",
    "    mean_f = mean_f.mean()\n",
    "    mean_supp = mean_supp.mean()\n",
    "    return np.array([mean_prec, mean_rec, mean_f, mean_supp], dtype=np.float64) / (i+1)\n",
    "    #print(f'Mean score over validation: {mean_score} using {method}')\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7948bd-2fc9-4dd5-ad42-ee831d6463fa",
   "metadata": {},
   "source": [
    "## P7\n",
    "Compare the classification performance of \"doing nothing\", 7 undersampling,4 oversampling, and 2 ensemble based methods in presence of class imbalance. Which method works generally best and the worst? Why?\n",
    "\n",
    "Since we are interested in measuring our performance on the minority class, we use precision-recall curve.\n",
    "First we get the training and testing data, and apply the sampling method on only the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d36b3d6-3db4-46c0-958b-ad686510347c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seeing: dataset: 3\n",
      "\tUndersampling\n",
      "\tnearmiss mean precision: 0.700, mean recall: 0.694 mean fscore: 0.691, mean support: 65.333\n",
      "\tclustercentroids mean precision: 0.932, mean recall: 0.929 mean fscore: 0.928, mean support: 65.333\n",
      "\tcondensednn mean precision: 0.664, mean recall: 0.616 mean fscore: 0.619, mean support: 93.667\n",
      "\teditednn mean precision: 0.678, mean recall: 0.546 mean fscore: 0.549, mean support: 214.167\n",
      "\tneighbourhoodcleaning mean precision: 0.657, mean recall: 0.540 mean fscore: 0.540, mean support: 213.500\n",
      "\ttomeklinks mean precision: 0.704, mean recall: 0.540 mean fscore: 0.544, mean support: 269.000\n",
      "\tonesidedselection mean precision: 0.540, mean recall: 0.506 mean fscore: 0.490, mean support: 268.333\n",
      "\n",
      "\tOversampling\n",
      "\tsmote mean precision: 0.662, mean recall: 0.531 mean fscore: 0.531, mean support: 268.833\n",
      "\tborderlinesmote mean precision: 0.560, mean recall: 0.509 mean fscore: 0.492, mean support: 266.500\n",
      "\tadasyn mean precision: 0.644, mean recall: 0.524 mean fscore: 0.519, mean support: 267.333\n",
      "\tsvmsmote mean precision: 0.662, mean recall: 0.529 mean fscore: 0.528, mean support: 268.667\n",
      "Seeing: biodeg.csv: 3\n",
      "\tUndersampling\n",
      "\tnearmiss mean precision: 0.802, mean recall: 0.801 mean fscore: 0.800, mean support: 118.667\n",
      "\tclustercentroids mean precision: 0.844, mean recall: 0.841 mean fscore: 0.841, mean support: 118.667\n",
      "\tcondensednn mean precision: 0.779, mean recall: 0.758 mean fscore: 0.764, mean support: 96.833\n",
      "\teditednn mean precision: 0.899, mean recall: 0.899 mean fscore: 0.899, mean support: 135.000\n",
      "\tneighbourhoodcleaning mean precision: 0.883, mean recall: 0.884 mean fscore: 0.883, mean support: 158.500\n",
      "\ttomeklinks mean precision: 0.857, mean recall: 0.859 mean fscore: 0.858, mean support: 167.000\n",
      "\tonesidedselection mean precision: 0.866, mean recall: 0.864 mean fscore: 0.864, mean support: 166.333\n",
      "\n",
      "\tOversampling\n",
      "\tsmote mean precision: 0.861, mean recall: 0.857 mean fscore: 0.859, mean support: 166.333\n",
      "\tborderlinesmote mean precision: 0.858, mean recall: 0.861 mean fscore: 0.859, mean support: 165.500\n",
      "\tadasyn mean precision: 0.852, mean recall: 0.852 mean fscore: 0.852, mean support: 165.000\n",
      "\tsvmsmote mean precision: 0.858, mean recall: 0.855 mean fscore: 0.855, mean support: 165.333\n",
      "Seeing: FuelConsumption.csv: 3\n",
      "\tUndersampling\n",
      "\tnearmiss mean precision: 0.830, mean recall: 0.825 mean fscore: 0.822, mean support: 19.000\n",
      "\tclustercentroids mean precision: 0.539, mean recall: 0.537 mean fscore: 0.527, mean support: 19.000\n",
      "\tcondensednn mean precision: 0.495, mean recall: 0.496 mean fscore: 0.479, mean support: 15.067\n",
      "\teditednn mean precision: 0.919, mean recall: 0.911 mean fscore: 0.914, mean support: 29.133\n",
      "\tneighbourhoodcleaning mean precision: 0.862, mean recall: 0.855 mean fscore: 0.857, mean support: 44.733\n",
      "\ttomeklinks mean precision: 0.808, mean recall: 0.795 mean fscore: 0.799, mean support: 55.667\n",
      "\tonesidedselection mean precision: 0.824, mean recall: 0.819 mean fscore: 0.817, mean support: 49.467\n",
      "\n",
      "\tOversampling\n",
      "\tsmote mean precision: 0.817, mean recall: 0.801 mean fscore: 0.806, mean support: 49.733\n",
      "\tborderlinesmote mean precision: 0.807, mean recall: 0.804 mean fscore: 0.803, mean support: 48.933\n",
      "\tadasyn mean precision: 0.800, mean recall: 0.794 mean fscore: 0.795, mean support: 50.533\n",
      "\tsvmsmote mean precision: 0.796, mean recall: 0.787 mean fscore: 0.786, mean support: 52.467\n",
      "Seeing: Travel_insurance.csv: 3\n",
      "\tUndersampling\n",
      "\tnearmiss mean precision: 0.756, mean recall: 0.749 mean fscore: 0.747, mean support: 236.667\n",
      "\tclustercentroids mean precision: 0.764, mean recall: 0.762 mean fscore: 0.761, mean support: 236.667\n",
      "\tcondensednn mean precision: 0.691, mean recall: 0.695 mean fscore: 0.690, mean support: 205.667\n",
      "\teditednn mean precision: 0.750, mean recall: 0.750 mean fscore: 0.745, mean support: 218.667\n",
      "\tneighbourhoodcleaning mean precision: 0.742, mean recall: 0.742 mean fscore: 0.737, mean support: 216.000\n",
      "\ttomeklinks mean precision: 0.801, mean recall: 0.769 mean fscore: 0.778, mean support: 299.167\n",
      "\tonesidedselection mean precision: 0.796, mean recall: 0.764 mean fscore: 0.772, mean support: 298.667\n",
      "\n",
      "\tOversampling\n",
      "\tsmote mean precision: 0.802, mean recall: 0.769 mean fscore: 0.778, mean support: 298.000\n",
      "\tborderlinesmote mean precision: 0.792, mean recall: 0.761 mean fscore: 0.769, mean support: 298.833\n",
      "\tadasyn mean precision: 0.815, mean recall: 0.773 mean fscore: 0.783, mean support: 298.333\n",
      "\tsvmsmote mean precision: 0.811, mean recall: 0.772 mean fscore: 0.781, mean support: 298.500\n",
      "Seeing: ECommerce.csv: 3\n",
      "\tUndersampling\n",
      "\tnearmiss mean precision: 0.587, mean recall: 0.580 mean fscore: 0.570, mean support: 1478.667\n",
      "\tclustercentroids mean precision: 0.722, mean recall: 0.702 mean fscore: 0.695, mean support: 1478.667\n",
      "\tcondensednn mean precision: 0.520, mean recall: 0.509 mean fscore: 0.478, mean support: 1115.000\n",
      "\teditednn mean precision: 0.966, mean recall: 0.954 mean fscore: 0.959, mean support: 1313.667\n",
      "\tneighbourhoodcleaning mean precision: 0.919, mean recall: 0.899 mean fscore: 0.903, mean support: 1385.667\n",
      "\ttomeklinks mean precision: 0.754, mean recall: 0.748 mean fscore: 0.735, mean support: 1651.667\n",
      "\tonesidedselection mean precision: 0.755, mean recall: 0.749 mean fscore: 0.737, mean support: 1651.667\n",
      "\n",
      "\tOversampling\n",
      "\tsmote mean precision: 0.698, mean recall: 0.663 mean fscore: 0.657, mean support: 1369.167\n",
      "\tborderlinesmote mean precision: 0.711, mean recall: 0.675 mean fscore: 0.669, mean support: 1382.500\n",
      "\tadasyn mean precision: 0.760, mean recall: 0.755 mean fscore: 0.742, mean support: 1651.500\n",
      "\tsvmsmote mean precision: 0.756, mean recall: 0.750 mean fscore: 0.737, mean support: 1651.167\n",
      "Seeing: heart_2020_cleaned.csv: 3\n",
      "\tUndersampling\n",
      "\tnearmiss mean precision: 0.782, mean recall: 0.782 mean fscore: 0.782, mean support: 9939.667\n"
     ]
    }
   ],
   "source": [
    "under_sampling_methods = ['nearmiss',\n",
    "                          'clustercentroids',\n",
    "                          'condensednn',\n",
    "                          'editednn',\n",
    "                          'neighbourhoodcleaning',\n",
    "                          'tomeklinks',\n",
    "                          'onesidedselection'\n",
    "                         ]\n",
    "\n",
    "over_sampling_methods = ['smote',\n",
    "                         'borderlinesmote',\n",
    "                         'adasyn',\n",
    "                         'svmsmote'\n",
    "                        ]\n",
    "\n",
    "for file in files:\n",
    "    df = load(file, False if file in no_headers else True)\n",
    "    # Encode df\n",
    "    df = encode_categorical(df)\n",
    "    # Split into testing and training set\n",
    "    x, y = df.iloc[:, :-1], df.iloc[:, -1]\n",
    "    print(f'Seeing: {file}: {get_least_frequent(y)}')\n",
    "\n",
    "    if get_least_frequent(y) < 2:\n",
    "        print('Cannot sample with less than two classes.')\n",
    "        continue\n",
    "    #x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, shuffle=True)\n",
    "    \n",
    "    # Undersample\n",
    "    print('\\tUndersampling')\n",
    "    for us_method in under_sampling_methods:\n",
    "        x_sample, y_sample  = get_imb_sample(x, y, s_method=us_method)\n",
    "        scores = train_imb(x_sample, y_sample, model='forest')\n",
    "        print(f'\\t{us_method} mean precision: {scores[0]:.3f}, mean recall: {scores[1]:.3f} mean fscore: {scores[2]:.3f}, mean support: {scores[3]:.3f}')\n",
    "    print('\\n\\tOversampling')\n",
    "    for os_method in over_sampling_methods:\n",
    "        # Do the same thing\n",
    "        x_sample, y_sample = get_imb_sample(x, y, s_method=os_method)\n",
    "        scores = train_imb(x_sample, y_sample, model='forest')\n",
    "        print(f'\\t{os_method} mean precision: {scores[0]:.3f}, mean recall: {scores[1]:.3f} mean fscore: {scores[2]:.3f}, mean support: {scores[3]:.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f01f5aa-126c-45d8-8cc5-0090af5b91af",
   "metadata": {},
   "source": [
    "## P8\n",
    "Can you find the datasets on which SMOTE-based oversampling works best? What does the data look like? Why?\n",
    "\n",
    "We can use the results from the previous problem, and describe each of the dataframes to find some patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33825105-7a3a-48b3-9542-dd274479d07e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e691d563-f30f-44cb-8383-e0af3015f76c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
